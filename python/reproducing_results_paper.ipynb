{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial GPEs \n",
    "\n",
    "We compute the $R^2$ and $\\% ISE$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics on test set for GPE trained for the output LVV:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output RVV:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output LAV:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output RAV:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output LVOTdiam:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output RVOTdiam:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output LVmass:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output LVWT:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output LVEDD:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output SeptumWT:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output RVlongdiam:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output TAT:\n",
      "  R2 = 1.00\n",
      "  %ISE = 100.00 %\n",
      "\n",
      "\n",
      "Statistics on test set for GPE trained for the output TATLVendo:\n",
      "  R2 = 0.99\n",
      "  %ISE = 100.00 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "R2score_vec, ISE_vec = postprocessing.compute_R2_ISE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The maximum scores were achieved for all the emulators, except for the emulator of the TAT that achieved \\(R^2=0.99\\) and TATLV\\textsubscript{endo} that achieved a \\(ISE=98.86\\%\\). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Sensitivity Analysis\n",
    "\n",
    "> First-order effects of the GSA when modifying anatomical and functional parameters are presented in Fig 5.4.\n",
    "\n",
    "<img src=\"./figures/GSA1_small.png\"  width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above was built in PPT, putting together individual pies. To print all the single pies, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "import numpy as np\n",
    "\n",
    "input_labels=np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/param_labels.txt\", dtype=\"str\", delimiter=\"\\n\")\n",
    "postprocessing.full_GSA(subfolder=\"initial_sweep\", output_labels_dir=\"/media/crg17/Seagate Expansion Drive/biomarkers_labels.txt\", input_labels=input_labels, first_order=True, second_order=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in ```/media/crg17/Seagate Expansion Drive/initial_sweep/figures```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In Fig \\ref{fig:GSA2}, we show the second-order interactions for each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/GSA2_small.png\"  width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above was built in PPT, putting together individual networks. The text in the centre of each network is also written manually. To print all the single networs, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "import numpy as np\n",
    "\n",
    "input_labels=np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/param_labels.txt\", dtype=\"str\", delimiter=\"\\n\")\n",
    "postprocessing.full_GSA(subfolder=\"initial_sweep\", output_labels_dir=\"/media/crg17/Seagate Expansion Drive/biomarkers_labels.txt\", input_labels=input_labels, first_order=False, second_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in ```/media/crg17/Seagate Expansion Drive/initial_sweep/figures```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration to literature data\n",
    "\n",
    "> We extracted these biomarkers from the NORRE study\\cite{Kou2014EchocardiographicStudy}, and ran three waves of BHM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the three waves using literature data (* **heavy computationally** *):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.literature(run_wave0=True, run_wave1=True, run_wave2=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">By the last wave, the original space was reduced to \\(9.55\\%\\) of the original space size, with a maximum uncertainty quotient of \\(1.9\\), and a median value of \\(0.67\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the numbers above we run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/initial_sweep/NROY_rel_literature.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/literature/wave1/NROY_rel_literature.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/literature/wave2/NROY_rel_literature.dat\", dtype=float)\n",
    "\n",
    "total_reduction = round(wave0_NROY_rel*1e-4*wave1_NROY_rel*wave2_NROY_rel,2)\n",
    "\n",
    "uncertainty_quotient_vec = np.genfromtxt(\"/media/crg17/Seagate Expansion Drive/literature/wave2/variance_quotient_wave2_literature.dat\", dtype=float)\n",
    "\n",
    "print(total_reduction)\n",
    "print(np.max(uncertainty_quotient_vec))\n",
    "print(np.median(uncertainty_quotient_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In Fig 5.6 we show how the biomarkers obtained when simulating and emulating in the NROY region fall better within the range reported in the literature, as we run more waves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/output_evolution_small.png\"  width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above was built in PPT, putting together individual plots. To print all the single plots, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.plot_output_evolution_seaborn(first_wave = 0, last_wave = 2,\n",
    "                                   subfolder = \"literature\", only_feasible = False,\n",
    "                                   output_labels_dir = \"/media/crg17/Seagate Expansion Drive/biomarkers_labels.txt\",\n",
    "                                   exp_mean_name = \"anatomy_EP_lit_mean.txt\",\n",
    "                                   exp_std_name = \"anatomy_EP_lit_sd.txt\",\n",
    "                                   units_dir = \"/media/crg17/Seagate Expansion Drive/biomarkers_units.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in ```/media/crg17/Seagate Expansion Drive/literature/figures```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration to synthetic patient data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the four waves using data from subject #01 (* **heavy computationally** *):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.patient(patient_number=1, run_wave0=True, run_wave1=True, run_wave2=True, run_wave3=True, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compared to the initial space, the biggest reduction in NROY size happened in the first wave, with a reduction down to \\(11.06\\%\\) of the original space. In the third wave, the space was reduced to \\(8.89\\%\\) of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave1/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(wave0_NROY_rel)\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.),2))\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using as ground truth the biomarkers extracted from that mesh, the implausibility of that point in the third wave was \\(0.9\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the case of subject \\(\\#01\\) in the third wave, although the maximum value of the variance quotient across all space was \\(2.23\\), the median value was \\(1.08\\), suggesting that the uncertainties of the emulators are comparable to the uncertainties of the ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/variance_quotient_wave2_patient1_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing emulators with a different subject\n",
    "\n",
    "> Firstly we ran the BHM pipeline for \\(3\\) waves with patient \\(\\#02\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the three waves using data from subject #02 (* **heavy computationally** *):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.patient(patient_number=2, run_wave0=False, run_wave1=False, run_wave2=True, run_wave3=False, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this case the NROY size was \\(9.49\\%\\) of the original space, and we achieve a final NROY size of \\(7.51\\%\\) of the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave1/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(wave0_NROY_rel)\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The implausibility for the known target parameter point was of \\(0.2\\),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/implausibilities_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and the maximum and median variance quotient (as defined in Eq \\ref{eq:VQ}) were of \\(2.2\\) and \\(0.72\\), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/variance_quotient_wave2_patient2_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We then used the emulators from the third wave of subject \\(\\#01\\), but using the ground truth data (the simulation results) from subject \\(\\#02\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.mix_patients(use_emulators_from_patient=1, new_patient=2, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Using the previously calibrated emulator, we obtained a NROY size of \\(7.13\\%\\) of the original size without running any extra waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/NROY_rel_patient2_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The implausibility of the point was \\(0.7\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/implausibilities_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The uncertainties were smaller, with a maximum and median of \\(1.94\\) and \\(0.94\\), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/variance_quotient_wave2_patient2_using_patient1_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Conversely, we tested the emulators from the third wave of subject \\(\\#02\\), but on subject \\(\\#01\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.mix_patients(use_emulators_from_patient=2, new_patient=1, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using these already trained emulators, without running any extra waves, we obtained a NROY size of \\(8.34\\%\\) of the original size (only \\(0.55\\) points of difference compared to the full pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient2_sd_10/wave2/NROY_rel_patient1_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave1/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "NROY_rel_full_pipeline = round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2)\n",
    "\n",
    "print(round(NROY_rel - NROY_rel_full_pipeline,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The implausibility of the point was \\(0.7\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient2_sd_10/wave2/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and the maximum and median uncertainties were of \\(2.51\\) and \\(0.82\\), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient2_sd_10/wave2/variance_quotient_wave2_patient1_using_patient2_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Subject | Using results from #01 | Using results from #02\n",
    "| :-: | \n",
    "| NROY size as % of original space | #01 | 8.89 | 8.34 |\n",
    "| NROY size as % of original space | #02 | 7.13 | 7.51 |\n",
    "| Implausibility of simulated point | #01 | 0.9 | 0.7 |\n",
    "| Implausibility of simulated point | #02 | 0.7 | 0.2 |\n",
    "| Uncertainty quotient Max/median | #01 | 2.23 / 1.08 | 2.51 / 0.82 |\n",
    "| Uncertainty quotient Max/median | #02 | 1.94 / 0.94 | 2.2 / 0.72 |\n",
    "| NROY+RO / NROY match | #01 | - | 98.95% / 95.75% |\n",
    "| NROY+RO / NROY match | #02 | 99.22% / 96.29% | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above have been mostly generated in previous snippets. The missing numbers are the NROY+RO / NROY match. They are calculated in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc_1_using_2_RO = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=1, original_last_wave=2, using_patient=2,\n",
    "                        using_last_wave=2)\n",
    "perc_1_using_2_NROY = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=1, original_last_wave=2, using_patient=2,\n",
    "                        using_last_wave=2)\n",
    "perc_2_using_1_RO = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=2, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "perc_2_using_1_NROY = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=2, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "\n",
    "print(round(perc_1_using_2_RO,2), round(perc_1_using_2_NROY,2), round(perc_2_using_1_RO,2), round(perc_2_using_1_NROY,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing emulators with very different subjects\n",
    "\n",
    "> In the first scenario, we computed the $\\ell1$ distance between the input vector of subject \\(\\#01\\) and the input vector of each other subject of the cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CT_cohort\n",
    "\n",
    "CT_cohort.generate_patient_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> reusing the third wave of patient \\(\\#01\\) using the biomarkers of this new subject, lead to similar results as running the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.run_farthest_patients(patient_number=1,input_or_output=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The NROY space was of \\(11.5\\%\\) without running any extra waves in the reusing scenario,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/NROY_rel_patient10_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> compared to \\(12.2\\%\\) in the full-pipeline scenario (after three waves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave1/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "NROY_rel_full_pipeline = round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2)\n",
    "\n",
    "print(NROY_rel_full_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the implausibility for that subject was of \\(0.39\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/implausibilities_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and the uncertainty quotients had a maximum of \\(2.44\\) and a median \\(1.1\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/variance_quotient_wave2_patient10_using_patient1_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> compared to an implausibility of \\(0.34\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/implausibilities_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and uncertainty quotients with a maximum of \\(2.02\\) and median of \\(1.18\\) if using the specialised pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/variance_quotient_wave2_patient10_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the second scenario, we computed the $\\ell1$ distance between the vector of biomarkers of subject \\(\\#01\\) and the vector of biomarkers of each other subject in the cohort. The farthest subject in this case was subject \\(\\#18\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.run_farthest_patients(patient_number=1,input_or_output=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using the results of the last wave of \\(\\#01\\) with the biomarkers of \\(\\#18\\) resulted in a reduction of the original space down to \\(1.94\\%\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/NROY_rel_patient18_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The uncertainty quotient in the third wave of \\(\\#18\\) had a maximum value of \\(3.64\\) and a median value of \\(1.64\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/variance_quotient_wave2_patient18_using_patient1_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The lowest match in terms of the NROY region similarity was also achieved when using the third wave of \\(\\#01\\) with the biomarkers of \\(\\#18\\): \\(83.61\\%\\) of the NROY points in the case of using the emulators of \\(\\#01\\) with the biomarkers of \\(\\#18\\) had the same status (not implausible) as with the case of running three waves using only case \\(\\#18\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=18, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "print(round(perc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | NROY size as % of original space | Implausibility of simulated point | Uncertainty quotient Max/median | NROY+RO / NROY match\n",
    "| :-: | \n",
    "| Using #10 on #01 | 8.73 | 0.7  |  2.43/1.31 | 99.06%/97.84% |\n",
    "| Ground truth | 8.89 |  0.9 | 2.23/1.08  | - |\n",
    "| Using #18 on #01 | 10.82 | 0.43 | 3.23/1.32  | 97.15%/97.71% |\n",
    "| Ground truth | 8.89 |  0.9 | 2.23/1.08  | - |\n",
    "| Using #01 on #10 | 11.5 | 0.39  | 2.44/1.1  | 98.97%/95.92% |\n",
    "| Ground truth | 12.2 | 0.34 |  2.02/1.18 | - |\n",
    "| Using #01 on #18 |  1.94 | 0.93  | 3.64/1.64 | 99.33%/83.61% |\n",
    "| Ground truth | 2.15 | 0.48 | 3.93/1.70  | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the numbers in the table have been already computed in previous snippets. The numbers of the first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient10_sd_10/wave2/NROY_rel_patient1_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient10_sd_10/wave2/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient10_sd_10/wave2/variance_quotient_wave2_patient1_using_patient10_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc1 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=1, original_last_wave=2, using_patient=10,\n",
    "                        using_last_wave=2)\n",
    "perc2 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=1, original_last_wave=2, using_patient=10,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "print(round(perc1,2),round(perc2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient18_sd_10/wave2/NROY_rel_patient1_sd_10_lhd_1000000.dat\", dtype=float)\n",
    "\n",
    "print(NROY_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient18_sd_10/wave2/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient18_sd_10/wave2/variance_quotient_wave2_patient1_using_patient18_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc1 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=1, original_last_wave=2, using_patient=18,\n",
    "                        using_last_wave=2)\n",
    "perc2 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=1, original_last_wave=2, using_patient=18,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "print(round(perc1,2),round(perc2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NROY match using #01 on #10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc1 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=10, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "perc2 = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=10, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "print(round(perc1,2),round(perc2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implausibility of simulated point using #01 on #18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/using_patient1_sd_10/wave2/implausibilities_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=18, original_last_wave=2, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "print(round(perc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave1/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "NROY_rel_full_pipeline = round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2)\n",
    "\n",
    "print(NROY_rel_full_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/implausibilities_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/variance_quotient_wave2_patient18_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the accuracy of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the effect now of using SD=5% of the mean. It should make the NROYs more personalised, and so increase the discrepancy between literature and personalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.patient(patient_number=1, run_wave0=True, run_wave1=True, run_wave2=True, run_wave3=True, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline_bhm\n",
    "\n",
    "pipeline_bhm.patient(patient_number=18, run_wave0=True, run_wave1=True, run_wave2=True, run_wave3=True, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the uncertainty of the simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the case of subject #01, the final NROY size went from 10.18% of the original space, in the last wave of the case of high uncertainty, down to 0.43%, in the first wave of the case of low uncertainty. By the third wave, the NROY space is 0.000096% of the original space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave1/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.*wave3_NROY_rel/100.),2))\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep_postthesis/NROY_rel_patient1_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_5/wave1/NROY_rel_patient1_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_5/wave2/NROY_rel_patient1_sd_5_lhd_100000.dat\", dtype=float)\n",
    "print(float(wave0_NROY_rel))\n",
    "print(float(wave0_NROY_rel*wave1_NROY_rel/100.))\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.*wave3_NROY_rel/100.),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Similarly, subject #02’s NROY changed from 7.26% to 0.00003%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave1/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2))\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient2_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_5/wave1/NROY_rel_patient2_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_5/wave2/NROY_rel_patient2_sd_5_lhd_100000.dat\", dtype=float)\n",
    "print(float(wave0_NROY_rel))\n",
    "print(float(wave0_NROY_rel*wave1_NROY_rel/100.))\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> subject #10, from 11.79% to 0.00003%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep_thesis/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave1/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.),2))\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient10_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_5/wave1/NROY_rel_patient10_sd_5_lhd_100000.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_5/wave2/NROY_rel_patient10_sd_5_lhd_100000.dat\", dtype=float)\n",
    "print(float(wave0_NROY_rel))\n",
    "print(float(wave0_NROY_rel*wave1_NROY_rel/100.))\n",
    "print(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> and subject #18, from 2.07% to 0.13%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave1/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.*wave3_NROY_rel/100.),2))\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep_postthesis/NROY_rel_patient18_sd_5_lhd_100000.dat\", dtype=float)\n",
    "#wave1_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_5/wave1/NROY_rel_patient18_sd_5_lhd_100000.dat\", dtype=float)\n",
    "#wave2_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_5/wave2/NROY_rel_patient18_sd_5_lhd_100000.dat\", dtype=float)\n",
    "print(float(wave0_NROY_rel))\n",
    "#print(float(wave0_NROY_rel*wave1_NROY_rel/100.))\n",
    "#print(round(float(wave0_NROY_rel*wave1_NROY_rel/100.*wave2_NROY_rel/100.*wave3_NROY_rel/100.),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In terms of VQ, the biggest increase was observed in subjects #01 where the maximum value of VQ changed from 2.92 to 14.03 and the median value changed from 1.11 to 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQ_p1_10 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/variance_quotient_wave2_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "VQ_p1_5 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_5/wave2/variance_quotient_wave2_patient1_sd_5.dat\", dtype=float)\n",
    "\n",
    "VQ_p2_10 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/variance_quotient_wave2_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "VQ_p2_5 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_5/wave2/variance_quotient_wave2_patient2_sd_5.dat\", dtype=float)\n",
    "\n",
    "VQ_p10_10 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/variance_quotient_wave2_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "VQ_p10_5 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_5/wave2/variance_quotient_wave2_patient10_sd_5.dat\", dtype=float)\n",
    "\n",
    "VQ_p18_10 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/variance_quotient_wave2_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "VQ_p18_5 = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/variance_quotient_wave0_patient18_sd_5.dat\", dtype=float)\n",
    "\n",
    "print(np.max(VQ_p1_5)-np.max(VQ_p1_10))\n",
    "print(np.median(VQ_p1_5)-np.median(VQ_p1_10))\n",
    "\n",
    "print(np.max(VQ_p2_5)-np.max(VQ_p2_10))\n",
    "print(np.median(VQ_p2_5)-np.median(VQ_p2_10))\n",
    "\n",
    "print(np.max(VQ_p10_5)-np.max(VQ_p10_10))\n",
    "print(np.median(VQ_p10_5)-np.median(VQ_p10_10))\n",
    "\n",
    "print(np.max(VQ_p18_5)-np.max(VQ_p18_10))\n",
    "print(np.median(VQ_p18_5)-np.median(VQ_p18_10))\n",
    "\n",
    "print(np.max(VQ_p1_10))\n",
    "print(np.max(VQ_p1_5))\n",
    "print(np.median(VQ_p1_10))\n",
    "print(np.median(VQ_p1_5))\n",
    "print(np.max(VQ_p2_10))\n",
    "print(np.max(VQ_p2_5))\n",
    "print(np.median(VQ_p2_10))\n",
    "print(np.median(VQ_p2_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_10/wave2/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient1_sd_5/wave2/implausibilities_patient1_sd_5.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_10/wave2/implausibilities_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient2_sd_5/wave2/implausibilities_patient2_sd_5.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_10/wave2/implausibilities_patient10_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient10_sd_5/wave2/implausibilities_patient10_sd_5.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/patient18_sd_10/wave2/implausibilities_patient18_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/implausibilities_patient18_sd_5.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient1_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient1_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient1_sd_10/wave1\", \"patient1_sd_10/wave2\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=10,\n",
    "                           folder_b=\"patient1_sd_5\", wave_b_number=2, wave_b_name=\"wave2_patient1_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\", \"patient1_sd_5/wave1\", \"patient1_sd_5/wave2\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=5)\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient2_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient2_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient2_sd_10/wave1\", \"patient2_sd_10/wave2\"],\n",
    "                           biomarkers_patient_number_a=2, sd_magnitude_a=10,\n",
    "                           folder_b=\"patient2_sd_5\", wave_b_number=2, wave_b_name=\"wave2_patient2_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\", \"patient2_sd_5/wave1\", \"patient2_sd_5/wave2\"],\n",
    "                           biomarkers_patient_number_b=2, sd_magnitude_b=5)\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient10_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient10_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient10_sd_10/wave1\", \"patient10_sd_10/wave2\"],\n",
    "                           biomarkers_patient_number_a=10, sd_magnitude_a=10,\n",
    "                           folder_b=\"patient10_sd_5\", wave_b_number=2, wave_b_name=\"wave2_patient10_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\", \"patient10_sd_5/wave1\", \"patient10_sd_5/wave2\"],\n",
    "                           biomarkers_patient_number_b=10, sd_magnitude_b=5)\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient18_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient18_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient18_sd_10/wave1\", \"patient18_sd_10/wave2\"],\n",
    "                           biomarkers_patient_number_a=18, sd_magnitude_a=10,\n",
    "                           folder_b=\"initial_sweep\", wave_b_number=0, wave_b_name=\"wave0_patient18_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\"],\n",
    "                           biomarkers_patient_number_b=18, sd_magnitude_b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find why the NROY of the L.U.C. is not completely included in the NROY of the H.U.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first approach, we check the overlapping of the NROY's using the emulators for the initial wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.46% of the space of initial_sweep matches that of initial_sweep\n",
      "89.46% of the space of initial_sweep matches that of initial_sweep\n",
      "23.22% of the NROY of initial_sweep matches that of initial_sweep\n",
      "100.0% of the NROY of initial_sweep matches that of initial_sweep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[89.46, 89.46, 23.22, 100.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"initial_sweep\", wave_a_number=0, wave_a_name=\"wave0_patient1_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=10,\n",
    "                           folder_b=\"initial_sweep\", wave_b_number=0, wave_b_name=\"wave0_patient1_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.13% of the space of patient1_sd_10 matches that of patient1_sd_5\n",
      "91.13% of the space of patient1_sd_5 matches that of patient1_sd_10\n",
      "13.59% of the NROY of patient1_sd_10 matches that of patient1_sd_5\n",
      "62.24% of the NROY of patient1_sd_5 matches that of patient1_sd_10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[91.13, 91.13, 13.59, 62.24]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient1_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient1_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient1_sd_10/wave1\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=10,\n",
    "                           folder_b=\"patient1_sd_5\", wave_b_number=2, wave_b_name=\"wave2_patient1_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\", \"patient1_sd_5/wave1\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.8% of the space of patient1_sd_10 matches that of patient1_sd_5\n",
      "90.8% of the space of patient1_sd_5 matches that of patient1_sd_10\n",
      "7.92% of the NROY of patient1_sd_10 matches that of patient1_sd_5\n",
      "55.14% of the NROY of patient1_sd_5 matches that of patient1_sd_10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[90.8, 90.8, 7.92, 55.14]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient1_sd_10\", wave_a_number=2, wave_a_name=\"wave2_patient1_sd_10\",\n",
    "                           emulators_a=[\"initial_sweep\", \"patient1_sd_10/wave1\", \"patient1_sd_10/wave2\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=10,\n",
    "                           folder_b=\"patient1_sd_5\", wave_b_number=2, wave_b_name=\"wave2_patient1_sd_5\",\n",
    "                           emulators_b=[\"initial_sweep\", \"patient1_sd_5/wave1\", \"patient1_sd_5/wave2\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it doesn't seem to be a bug. Looking at the VQ seems that the median variance also increases more in the L.U.C. and so points that before were implausible, now they are not implausible because we lose confindence in the ability of the emulator to predict. In fact, the variances are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median variances are [22.36, 29.6, 12.15, 12.88, 7.72, 8.1, 31.1, 2.36, 13.2, 2.14, 33.73, 17.15, 12.04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[22.361498,\n",
       " 29.59712,\n",
       " 12.148354,\n",
       " 12.879721,\n",
       " 7.7194386,\n",
       " 8.103218,\n",
       " 31.100185,\n",
       " 2.364392,\n",
       " 13.202198,\n",
       " 2.135342,\n",
       " 33.72928,\n",
       " 17.151297,\n",
       " 12.043879]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_emulator_variances(emulators_folders=[\"initial_sweep\", \"patient1_sd_10/wave1\", \"patient1_sd_10/wave2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in the L.U.C. the variances are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median variances are [24.0, 34.36, 17.51, 16.01, 8.75, 9.77, 36.73, 2.95, 13.66, 2.22, 35.98, 23.02, 10.81]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[24.002808,\n",
       " 34.36161,\n",
       " 17.505074,\n",
       " 16.005962,\n",
       " 8.749542,\n",
       " 9.767463,\n",
       " 36.727364,\n",
       " 2.9507396,\n",
       " 13.658985,\n",
       " 2.224835,\n",
       " 35.97692,\n",
       " 23.0153,\n",
       " 10.807833]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_emulator_variances(emulators_folders=[\"initial_sweep\", \"patient1_sd_5/wave1\", \"patient1_sd_5/wave2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median variances are [22.36, 29.6, 12.15, 12.88, 7.72, 8.1, 31.1, 2.36, 13.2, 2.14, 33.73, 17.15, 12.04]\n",
      "The median variances are [24.0, 34.36, 17.51, 16.01, 8.75, 9.77, 36.73, 2.95, 13.66, 2.22, 35.98, 23.02, 10.81]\n",
      "[7.34, 16.1, 44.09, 24.27, 13.34, 20.54, 18.09, 24.8, 3.46, 4.19, 6.66, 34.19, -10.26]\n",
      "There has been a mean change of 15.91%+-14%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import postprocessing\n",
    "\n",
    "HUC_var = postprocessing.print_emulator_variances(emulators_folders=[\"initial_sweep\", \"patient1_sd_10/wave1\", \"patient1_sd_10/wave2\"])\n",
    "LUC_var = postprocessing.print_emulator_variances(emulators_folders=[\"initial_sweep\", \"patient1_sd_5/wave1\", \"patient1_sd_5/wave2\"])\n",
    "\n",
    "delta_var = [100*(LUC_var[i]/HUC_var[i]-1) for i in range(len(HUC_var))]\n",
    "print([round(d,2) for d in delta_var])\n",
    "\n",
    "print(\"There has been a mean change of {}%+-{}%\".format(round(np.mean(delta_var),2),round(np.std(delta_var)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it seems that in the case of the ground truth points the situation changes, since in the H.U.C. they were non-implausible, while in the L.U.C. they are implausible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implausibilities are [0.04, 0.01, 0.01, 0.02, 0.01, 0.12, 0.03, 0.07, 0.08, 0.37, 0.01, 0.23, 0.46]\n",
      "The numerators are [0.41, 0.19, 0.09, 0.18, 0.03, 0.33, 0.58, 0.09, 0.33, 0.41, 0.05, 2.37, 6.0]\n",
      "The denominators are [11.66, 17.92, 7.06, 11.3, 2.42, 2.86, 16.94, 1.24, 4.32, 1.13, 8.68, 10.19, 13.12]\n",
      "The emulated results are [116.9, 179.15, 70.23, 112.97, 24.12, 27.96, 168.67, 12.22, 42.64, 11.57, 86.21, 85.17, 75.91]\n",
      "The emulated variances are [116.9, 179.15, 70.23, 112.97, 24.12, 27.96, 168.67, 12.22, 42.64, 11.57, 86.21, 85.17, 75.91]\n"
     ]
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient1_sd_10/wave1\", \"patient1_sd_10/wave2\"],\n",
    "                                       patient_number=1, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.18, 1.96, 2.23, 3.34, 1.06, 0.03, 1.96, 2.6, 0.59, 0.85, 1.52, 0.49, 0.52]\n",
      "The numerators are [1.43, 30.83, 20.31, 36.29, 1.68, 0.05, 27.42, 2.71, 1.41, 0.57, 8.79, 3.25, 3.48]\n",
      "The denominators are [7.88, 15.71, 9.1, 10.87, 1.59, 1.98, 13.97, 1.04, 2.4, 0.67, 5.78, 6.58, 6.62]\n",
      "The emulated results are [117.92, 148.13, 50.01, 76.5, 22.47, 28.24, 141.83, 9.6, 44.38, 11.73, 77.37, 79.55, 78.43]\n",
      "The emulated SDs are [5.31, 12.91, 8.4, 9.29, 1.03, 1.39, 11.11, 0.84, 1.08, 0.37, 3.85, 5.12, 5.21]\n"
     ]
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient1_sd_5/wave1\",\n",
    "                                                                     \"patient1_sd_5/wave2\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in the case of subject #01 the ground truth point becomes implausible because instead of predicting a RAV of 113 mL approx, it predicts 50.01 mL. We repeat the process for subject #02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient2_sd_10/wave1\",\n",
    "                                                                     \"patient2_sd_10/wave2\"],\n",
    "                                       patient_number=2, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient2_sd_5/wave1\",\n",
    "                                                                     \"patient2_sd_5/wave2\"],\n",
    "                                       patient_number=2, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the problem is with the RVOTdiam, predicting 28.45 mm instead of 35.54. As for case #10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient10_sd_10/wave1\",\n",
    "                                                                     \"patient10_sd_10/wave2\"],\n",
    "                                       patient_number=10, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\", \"patient10_sd_5/wave1\",\n",
    "                                                                     \"patient10_sd_5/wave2\"],\n",
    "                                       patient_number=10, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the problem is with LAV, predicting a volume of 45.3 mL, instead of 86 mL. So the problem in all cases is a loss of accuracy in the emulators. Since from the first wave there is a collapse of the NROY space and in new waves the emulators train in the neighbourhood of the NROY, the L.U.C. emulators do not have enough accuracy to predict points outside of the NROY and therefore being non-implausible. This fact provokes a domino effect on the shape of the NROY. A potential solution if using higher accuracy data acquisition techniques such as MRI would be to sample in a bigger space (not only in the neighborhood of the NROY region); reducing the number of training points in subsequent waves or run only a single wave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra edits before submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a column to Table 3 repeating the analysis but using the emulators only of the initial sweep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NROY size as % of original space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient1_sd_10.dat\", dtype=float)\n",
    "print(wave0_NROY_rel)\n",
    "\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient2_sd_10.dat\", dtype=float)\n",
    "print(wave0_NROY_rel)\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient10_sd_10.dat\", dtype=float)\n",
    "print(wave0_NROY_rel)\n",
    "\n",
    "wave0_NROY_rel = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/NROY_rel_patient18_sd_10.dat\", dtype=float)\n",
    "print(wave0_NROY_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implausibility of simulated point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/implausibilities_patient1_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))\n",
    "\n",
    "implausibility = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/implausibilities_patient2_sd_10.dat\", dtype=float)\n",
    "\n",
    "print(np.max(implausibility))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$VQ$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/variance_quotient_wave0_patient1_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQ = np.genfromtxt(\"/media/crg17/SeagateExpansionDrive/initial_sweep/variance_quotient_wave0_patient2_sd_10.dat\", dtype=float)\n",
    "print(np.max(VQ))\n",
    "print(np.median(VQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NROY+RO/NROY match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "\n",
    "perc_initial_sweep_using_2_RO = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=1, original_last_wave=0, using_patient=2,\n",
    "                        using_last_wave=2)\n",
    "perc_initial_sweep_using_2_NROY = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=1, original_last_wave=0, using_patient=2,\n",
    "                        using_last_wave=2)\n",
    "perc_initial_sweep_using_1_RO = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=True, original_patient=2, original_last_wave=0, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "perc_initial_sweep_using_1_NROY = postprocessing.compare_nroy_binary(n_samples=1e5, whole_space=False, original_patient=2, original_last_wave=0, using_patient=1,\n",
    "                        using_last_wave=2)\n",
    "\n",
    "\n",
    "print(round(perc_initial_sweep_using_2_RO,2), round(perc_initial_sweep_using_2_NROY,2), round(perc_initial_sweep_using_1_RO,2), round(perc_initial_sweep_using_1_NROY,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to visualize the parameter space is drawing the distribution of parameter values in the NROY region. We used kernel densities estimations of histograms with a bin width of 5% the range of the values for each parameter. We first show initial space values values compared with subject #01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "import os\n",
    "from global_variables_config import *\n",
    "\n",
    "waves_paths = [os.path.join(PROJECT_PATH, \"initial_sweep\", \"wave0_patient1_sd_10\"),\n",
    "               os.path.join(PROJECT_PATH, \"patient1_sd_10\", \"wave1\", \"wave1_patient1_sd_10\"),\n",
    "               os.path.join(PROJECT_PATH, \"patient1_sd_10\", \"wave2\", \"wave2_patient1_sd_10\")\n",
    "             ]\n",
    "legends = [\"Using emulators from the first wave of #01\",\n",
    "           \"Using emulators from the second wave of #01\",\n",
    "           \"Using emulators from the third wave of #01\"\n",
    "          ]\n",
    "\n",
    "file_path = os.path.join(PROJECT_PATH, \"figures\")\n",
    "file_name = \"densities_waves_#01.png\"\n",
    "\n",
    "postprocessing.plot_parameter_distributions(waves_paths = waves_paths, legends = legends,\n",
    "                                            file_path = file_path, file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare the last wave of two very different patients and if we use the biomarkers of one with the emulators of the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import postprocessing\n",
    "import os\n",
    "from global_variables_config import *\n",
    "\n",
    "waves_paths = [os.path.join(PROJECT_PATH, \"patient1_sd_10\", \"wave2\", \"wave2_patient1_sd_10\"),\n",
    "               os.path.join(PROJECT_PATH, \"patient18_sd_10\", \"wave2\", \"wave2_patient18_sd_10\"),\n",
    "               os.path.join(PROJECT_PATH, \"using_patient1_sd_10\", \"wave2\", \"wave2_patient18_using_patient1_sd_10\")\n",
    "             ]\n",
    "legends = [\"Trained with data from #01\\n and constraining with data from #01\",\n",
    "           \"Trained with data from #18\\n and constraining with data from #18\",\n",
    "           \"Trained with data from #01\\n and constraining with data from #18\",\n",
    "          ]\n",
    "\n",
    "file_path = os.path.join(PROJECT_PATH, \"figures\")\n",
    "file_name = \"densities_#01_vs_#18.png\"\n",
    "\n",
    "postprocessing.plot_parameter_distributions(waves_paths = waves_paths, legends = legends,\n",
    "                                            file_path = file_path, file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a 1D example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesise that training with high density of points in a region compared to low density outside increases the uncertainty in the middle. To test this we try to reproduce it using a 1D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to learn (normally a high-dimensional, expensive deterministic model)\n",
    "from GPErks.utils.test_functions import forrester\n",
    "f = lambda x: np.sin(20*x)\n",
    "D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from GPErks.gp.data.dataset import Dataset\n",
    "dataset_1 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=3,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[0],\n",
    "    u_bounds=[0.5] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add extra point at the end\n",
    "\n",
    "dataset_2 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=4,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[0.8],\n",
    "    u_bounds=[1]  \n",
    ")\n",
    "\n",
    "\n",
    "# we merge them\n",
    "\n",
    "dataset = dataset_1\n",
    "\n",
    "dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train])\n",
    "dataset.y_train = np.append(dataset_1.y_train,dataset_2.y_train)\n",
    "\n",
    "dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size\n",
    "\n",
    "dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0]])]\n",
    "dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0]])]\n",
    "\n",
    "# choose likelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "likelihood = GaussianLikelihood()\n",
    "\n",
    "# choose mean function\n",
    "from gpytorch.means import LinearMean\n",
    "mean_function = LinearMean(input_size=dataset.input_size)\n",
    "\n",
    "# choose covariance function (kernel)\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "kernel = ScaleKernel(RBFKernel(ard_num_dims=dataset.input_size))\n",
    "\n",
    "# choose metrics\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "metrics = [MeanSquaredError(), R2Score()]\n",
    "\n",
    "# define experiment\n",
    "from GPErks.gp.experiment import GPExperiment\n",
    "experiment = GPExperiment(\n",
    "    dataset,\n",
    "    likelihood,\n",
    "    mean_function,\n",
    "    kernel,\n",
    "    n_restarts=3,\n",
    "    metrics=metrics,\n",
    "    seed=2,  # reproducible training\n",
    "    learn_noise=True  # y = f(x) + e, e ~ N(0, sigma^2I)\n",
    ")\n",
    "\n",
    "# train model\n",
    "from GPErks.train.emulator import GPEmulator\n",
    "import torch\n",
    "emulator = GPEmulator(experiment, device=\"cpu\")\n",
    "optimizer = torch.optim.Adam(experiment.model.parameters(), lr=0.1)\n",
    "emulator.train(optimizer)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = dataset.X_train\n",
    "y_train = dataset.y_train\n",
    "\n",
    "xx = np.linspace(dataset.l_bounds[0], dataset.u_bounds[0], 1000)\n",
    "\n",
    "yy_mean, yy_std = emulator.predict(xx)\n",
    "yy_true = f(xx)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "height = 9.36111\n",
    "width = 5.91667\n",
    "fig, axis = plt.subplots(1, 1, figsize=(4*width/3, height/2))\n",
    "\n",
    "axis.plot(xx, yy_true, c=\"C0\", ls=\"--\", label=\"true function\")\n",
    "\n",
    "CI = 2\n",
    "axis.plot(xx, yy_mean, c=\"C0\", label=\"predicted mean\")\n",
    "axis.fill_between(\n",
    "    xx, yy_mean - CI * yy_std, yy_mean + CI * yy_std, color=\"C0\", alpha=0.15, label=\"~95% CI\"\n",
    ")\n",
    "axis.scatter(x_train, y_train, fc=\"C0\", ec=\"C0\", label=\"training data\")\n",
    "axis.set_ylim([-2,4])\n",
    "axis.legend(loc=\"best\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(yy_std[200:900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add extra point at the end\n",
    "\n",
    "dataset_2 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=10,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[0.8],\n",
    "    u_bounds=[1]  \n",
    ")\n",
    "\n",
    "\n",
    "# we merge them\n",
    "\n",
    "dataset = dataset_1\n",
    "\n",
    "dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train])\n",
    "dataset.y_train = np.append(dataset_1.y_train,dataset_2.y_train)\n",
    "\n",
    "dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size\n",
    "\n",
    "dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0]])]\n",
    "dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0]])]\n",
    "\n",
    "# choose likelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "likelihood = GaussianLikelihood()\n",
    "\n",
    "# choose mean function\n",
    "from gpytorch.means import LinearMean\n",
    "mean_function = LinearMean(input_size=dataset.input_size)\n",
    "\n",
    "# choose covariance function (kernel)\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "kernel = ScaleKernel(RBFKernel(ard_num_dims=dataset.input_size))\n",
    "\n",
    "# choose metrics\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "metrics = [MeanSquaredError(), R2Score()]\n",
    "\n",
    "# define experiment\n",
    "from GPErks.gp.experiment import GPExperiment\n",
    "experiment = GPExperiment(\n",
    "    dataset,\n",
    "    likelihood,\n",
    "    mean_function,\n",
    "    kernel,\n",
    "    n_restarts=3,\n",
    "    metrics=metrics,\n",
    "    seed=2,  # reproducible training\n",
    "    learn_noise=True  # y = f(x) + e, e ~ N(0, sigma^2I)\n",
    ")\n",
    "\n",
    "# train model\n",
    "from GPErks.train.emulator import GPEmulator\n",
    "import torch\n",
    "emulator = GPEmulator(experiment, device=\"cpu\")\n",
    "optimizer = torch.optim.Adam(experiment.model.parameters(), lr=0.1)\n",
    "emulator.train(optimizer)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = dataset.X_train\n",
    "y_train = dataset.y_train\n",
    "\n",
    "xx = np.linspace(dataset.l_bounds[0], dataset.u_bounds[0], 1000)\n",
    "\n",
    "yy_mean, yy_std = emulator.predict(xx)\n",
    "yy_true = f(xx)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "height = 9.36111\n",
    "width = 5.91667\n",
    "fig, axis = plt.subplots(1, 1, figsize=(4*width/3, height/2))\n",
    "\n",
    "axis.plot(xx, yy_true, c=\"C0\", ls=\"--\", label=\"true function\")\n",
    "\n",
    "CI = 2\n",
    "axis.plot(xx, yy_mean, c=\"C0\", label=\"predicted mean\")\n",
    "axis.fill_between(\n",
    "    xx, yy_mean - CI * yy_std, yy_mean + CI * yy_std, color=\"C0\", alpha=0.15, label=\"~95% CI\"\n",
    ")\n",
    "axis.scatter(x_train, y_train, fc=\"C0\", ec=\"C0\", label=\"training data\")\n",
    "axis.set_ylim([-2,4])\n",
    "axis.legend(loc=\"best\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(yy_std[200:900]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make the uncertainty not capture the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import numpy as np\n",
    "    \n",
    "# function to learn (normally a high-dimensional, expensive deterministic model)\n",
    "from GPErks.utils.test_functions import forrester\n",
    "# a_sin = 120.\n",
    "# f = lambda x: np.sin(a_sin*x)\n",
    "alpha_fun = 0.4\n",
    "omega_fun = 20\n",
    "beta_fun = 0.4\n",
    "c_fun = -0.6\n",
    "f = lambda x: alpha_fun*np.cos((omega_fun*x)/beta_fun)-c_fun*(x/beta_fun)**2\n",
    "D = 1\n",
    "\n",
    "\n",
    "from GPErks.gp.data.dataset import Dataset\n",
    "\n",
    "dataset_1 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=2,\n",
    "    n_test_samples=1,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[-1],\n",
    "    u_bounds=[-0.75] \n",
    ")\n",
    "\n",
    "# we add extra point at the end\n",
    "\n",
    "dataset_2 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=2,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[0.93],\n",
    "    u_bounds=[1]  \n",
    ")\n",
    "\n",
    "dataset_3 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=2,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=1,\n",
    "#     l_bounds=[2*2*np.pi/a_sin-0.01-0.25*2*np.pi/a_sin-0.055],\n",
    "#     u_bounds=[2*2*np.pi/a_sin+0.01-0.25*2*np.pi/a_sin-0.055]  \n",
    "    l_bounds = [-0.35],\n",
    "    u_bounds = [-0.3]\n",
    ")\n",
    "\n",
    "\n",
    "# we merge them\n",
    "\n",
    "dataset = dataset_1\n",
    "\n",
    "dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train,dataset_3.X_train])\n",
    "dataset.y_train = np.concatenate((dataset_1.y_train,dataset_2.y_train,dataset_3.y_train))\n",
    "\n",
    "dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size + dataset_3.sample_size\n",
    "\n",
    "dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0],dataset_3.l_bounds[0]])]\n",
    "dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0],dataset_3.u_bounds[0]])]\n",
    "\n",
    "# dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train])\n",
    "# dataset.y_train = np.append(dataset_1.y_train,[dataset_2.y_train])\n",
    "\n",
    "# dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size \n",
    "\n",
    "# dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0]])]\n",
    "# dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0]])]\n",
    "\n",
    "# choose likelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "likelihood = GaussianLikelihood()\n",
    "\n",
    "# choose mean function\n",
    "from gpytorch.means import LinearMean\n",
    "mean_function = LinearMean(input_size=dataset.input_size)\n",
    "\n",
    "# choose covariance function (kernel)\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "kernel = ScaleKernel(RBFKernel(ard_num_dims=dataset.input_size))\n",
    "\n",
    "# choose metrics\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "metrics = [MeanSquaredError(), R2Score()]\n",
    "\n",
    "# define experiment\n",
    "from GPErks.gp.experiment import GPExperiment\n",
    "experiment = GPExperiment(\n",
    "    dataset,\n",
    "    likelihood,\n",
    "    mean_function,\n",
    "    kernel,\n",
    "    n_restarts=3,\n",
    "    metrics=metrics,\n",
    "    seed=2,  # reproducible training\n",
    "    learn_noise=True  # y = f(x) + e, e ~ N(0, sigma^2I)\n",
    ")\n",
    "\n",
    "# train model\n",
    "from GPErks.train.emulator import GPEmulator\n",
    "import torch\n",
    "emulator = GPEmulator(experiment, device=\"cpu\")\n",
    "optimizer = torch.optim.Adam(experiment.model.parameters(), lr=0.1)\n",
    "emulator.train(optimizer)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = dataset.X_train\n",
    "y_train = dataset.y_train\n",
    "\n",
    "xx = np.linspace(dataset.l_bounds[0], dataset.u_bounds[0], 1000)\n",
    "\n",
    "yy_mean, yy_std = emulator.predict(xx)\n",
    "yy_true = f(xx)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "height = 9.36111\n",
    "width = 5.91667\n",
    "fig, axis = plt.subplots(1, 1, figsize=(4*width/3, height/2))\n",
    "\n",
    "axis.plot(xx, yy_true, c=\"C0\", ls=\"--\", label=\"true function\")\n",
    "\n",
    "CI = 3\n",
    "axis.plot(xx, yy_mean, c=\"C0\", label=\"predicted mean\")\n",
    "axis.fill_between(\n",
    "    xx, yy_mean - CI * yy_std, yy_mean + CI * yy_std, color=\"C0\", alpha=0.15, label=\"~95% CI\"\n",
    ")\n",
    "axis.scatter(x_train, y_train, fc=\"C0\", ec=\"C0\", label=\"training data\")\n",
    "axis.set_ylim([-1,4])\n",
    "axis.legend(loc=\"best\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(yy_std[200:900]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import numpy as np\n",
    "    \n",
    "# function to learn (normally a high-dimensional, expensive deterministic model)\n",
    "from GPErks.utils.test_functions import forrester\n",
    "# a_sin = 120.\n",
    "# f = lambda x: np.sin(a_sin*x)\n",
    "alpha_fun = 0.4\n",
    "omega_fun = 20\n",
    "beta_fun = 0.4\n",
    "c_fun = -0.6\n",
    "f = lambda x: alpha_fun*np.cos((omega_fun*x)/beta_fun)-c_fun*(x/beta_fun)**2\n",
    "D = 1\n",
    "\n",
    "\n",
    "from GPErks.gp.data.dataset import Dataset\n",
    "\n",
    "dataset_1 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=2,\n",
    "    n_test_samples=1,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[-1],\n",
    "    u_bounds=[-0.75] \n",
    ")\n",
    "\n",
    "# we add extra point at the end\n",
    "\n",
    "dataset_2 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=20,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=2,\n",
    "    l_bounds=[0.99],\n",
    "    u_bounds=[1]  \n",
    ")\n",
    "\n",
    "dataset_3 = Dataset.build_from_function(\n",
    "    f,\n",
    "    D,\n",
    "    n_train_samples=2,\n",
    "    n_test_samples=0,\n",
    "    design=\"srs\",\n",
    "    seed=1,\n",
    "#     l_bounds=[2*2*np.pi/a_sin-0.01-0.25*2*np.pi/a_sin-0.055],\n",
    "#     u_bounds=[2*2*np.pi/a_sin+0.01-0.25*2*np.pi/a_sin-0.055]  \n",
    "    l_bounds = [-0.35],\n",
    "    u_bounds = [-0.3]\n",
    ")\n",
    "\n",
    "\n",
    "# we merge them\n",
    "\n",
    "dataset = dataset_1\n",
    "\n",
    "dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train,dataset_3.X_train])\n",
    "dataset.y_train = np.concatenate((dataset_1.y_train,dataset_2.y_train,dataset_3.y_train))\n",
    "\n",
    "dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size + dataset_3.sample_size\n",
    "\n",
    "dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0],dataset_3.l_bounds[0]])]\n",
    "dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0],dataset_3.u_bounds[0]])]\n",
    "\n",
    "# dataset.X_train = np.vstack([dataset_1.X_train,dataset_2.X_train])\n",
    "# dataset.y_train = np.append(dataset_1.y_train,[dataset_2.y_train])\n",
    "\n",
    "# dataset.sample_size = dataset_1.sample_size + dataset_2.sample_size \n",
    "\n",
    "# dataset.l_bounds = [np.min([dataset_1.l_bounds[0],dataset_2.l_bounds[0]])]\n",
    "# dataset.u_bounds = [np.max([dataset_1.u_bounds[0],dataset_2.u_bounds[0]])]\n",
    "\n",
    "# choose likelihood\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "likelihood = GaussianLikelihood()\n",
    "\n",
    "# choose mean function\n",
    "from gpytorch.means import LinearMean\n",
    "mean_function = LinearMean(input_size=dataset.input_size)\n",
    "\n",
    "# choose covariance function (kernel)\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "kernel = ScaleKernel(RBFKernel(ard_num_dims=dataset.input_size))\n",
    "\n",
    "# choose metrics\n",
    "from torchmetrics import MeanSquaredError, R2Score\n",
    "metrics = [MeanSquaredError(), R2Score()]\n",
    "\n",
    "# define experiment\n",
    "from GPErks.gp.experiment import GPExperiment\n",
    "experiment = GPExperiment(\n",
    "    dataset,\n",
    "    likelihood,\n",
    "    mean_function,\n",
    "    kernel,\n",
    "    n_restarts=3,\n",
    "    metrics=metrics,\n",
    "    seed=2,  # reproducible training\n",
    "    learn_noise=True  # y = f(x) + e, e ~ N(0, sigma^2I)\n",
    ")\n",
    "\n",
    "# train model\n",
    "from GPErks.train.emulator import GPEmulator\n",
    "import torch\n",
    "emulator = GPEmulator(experiment, device=\"cpu\")\n",
    "optimizer = torch.optim.Adam(experiment.model.parameters(), lr=0.1)\n",
    "emulator.train(optimizer)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_train = dataset.X_train\n",
    "y_train = dataset.y_train\n",
    "\n",
    "xx = np.linspace(dataset.l_bounds[0], dataset.u_bounds[0], 1000)\n",
    "\n",
    "yy_mean, yy_std = emulator.predict(xx)\n",
    "yy_true = f(xx)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "height = 9.36111\n",
    "width = 5.91667\n",
    "fig, axis = plt.subplots(1, 1, figsize=(4*width/3, height/2))\n",
    "\n",
    "axis.plot(xx, yy_true, c=\"C0\", ls=\"--\", label=\"true function\")\n",
    "\n",
    "CI = 2\n",
    "axis.plot(xx, yy_mean, c=\"C0\", label=\"predicted mean\")\n",
    "axis.fill_between(\n",
    "    xx, yy_mean - CI * yy_std, yy_mean + CI * yy_std, color=\"C0\", alpha=0.15, label=\"~95% CI\"\n",
    ")\n",
    "axis.scatter(x_train, y_train, fc=\"C0\", ec=\"C0\", label=\"training data\")\n",
    "axis.set_ylim([-1,5])\n",
    "axis.legend(loc=\"best\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.max(yy_std[200:900]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra efforts to explain why LUC is not in HUC and why the ground truth points are not in the NROY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to find a more satisfactory explanation to the problems with the LUC, we added more cases to the global emulator. We merged manually the input points of the initial emulator with those of patients 2, 10 and 18 for waves 1 and 2. We did the same with all the biomarkers for a total of 1120 points, instead of 280. We now train the emulators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emulators\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we do the BHM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing implausibility of NROY...\n",
      "Finished\n",
      "Results saved in /media/crg17/SeagateExpansionDrive/initial_sweep_all_patients_but_1\n"
     ]
    }
   ],
   "source": [
    "import history_matching\n",
    "\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, implausibility_threshold=3.2,\n",
    "                                                    literature_data=False, input_folder=\"initial_sweep_all_patients_but_1\",\n",
    "                                                    patient_number=1, sd_magnitude=5,\n",
    "                                                    first_time=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_matching.plot_nroy(input_folder=\"initial_sweep_all_patients_but_1\", wave=wave, literature_data=False,\n",
    "                                   patient_number=1, sd_magnitude=5,\n",
    "                                   title = \"Initial wave for #1 with SD=5% using data from all previous patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, output_folder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                                                   input_folder=\"initial_sweep_all_patients_but_1\", wave_name=\"wave0_patient1_sd_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check how is it going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.58% of the space of initial_sweep_all_patients_but_1 matches that of initial_sweep\n",
      "89.58% of the space of initial_sweep matches that of initial_sweep_all_patients_but_1\n",
      "87.8% of the NROY of initial_sweep_all_patients_but_1 matches that of initial_sweep\n",
      "24.53% of the NROY of initial_sweep matches that of initial_sweep_all_patients_but_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[89.58, 89.58, 87.8, 24.53]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"initial_sweep_all_patients_but_1\", wave_a_number=0, wave_a_name=\"wave0_patient1_sd_5\",\n",
    "                           emulators_a=[\"initial_sweep_all_patients_but_1\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=5,\n",
    "                           folder_b=\"initial_sweep\", wave_b_number=0, wave_b_name=\"wave0_patient1_sd_10\",\n",
    "                           emulators_b=[\"initial_sweep\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.08, 0.03, 0.18, 0.04, 0.02, 0.08, 0.13, 0.17, 0.26, 0.65, 0.04, 1.51, 0.08]\n",
      "The numerators are [0.5, 0.23, 0.66, 0.25, 0.02, 0.12, 1.13, 0.11, 0.58, 0.38, 0.16, 10.21, 0.65]\n",
      "The denominators are [5.86, 8.99, 3.59, 5.69, 1.22, 1.49, 8.5, 0.64, 2.21, 0.59, 4.42, 6.75, 8.56]\n",
      "The emulated results are [116.99, 178.73, 69.66, 112.54, 24.17, 28.17, 168.12, 12.2, 42.39, 11.54, 86.0, 93.01, 82.56]\n",
      "The emulated SDs are [0.65, 0.9, 0.73, 0.77, 0.17, 0.47, 0.78, 0.19, 0.5, 0.19, 0.99, 5.33, 7.51]\n"
     ]
    }
   ],
   "source": [
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep_all_patients_but_1\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we cannot compare them directly, because the ground truth of the naive approach with HUC, has a different training set. We would need to run with SD=10% but with this new training set. Since the implausibilities of the ground truth points are at least under the threshold for now we continue with SD=5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_meshes\n",
    "import preprocess_mesh\n",
    "import ep_simulations\n",
    "import biomarkers\n",
    "import emulators\n",
    "import history_matching\n",
    "import os\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "print(\"Running wave 1...\")\n",
    "\n",
    "generate_meshes.sample_atlas(subfolder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                             csv_filename=\"input_anatomy_training.csv\")\n",
    "preprocess_mesh.biv_setup(subfolder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                          anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                          ep_dat_file=\"input_ep_training.dat\")\n",
    "ep_simulations.run(subfolder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "biomarkers.extract(subfolder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\"])\n",
    "history_matching.save_patient_implausibility(emulators_vector=emulators_vector, \n",
    "                                             input_folder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                                             patient_number=1, sd_magnitude=5)\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, \n",
    "                                            implausibility_threshold=3.2,\n",
    "                                            literature_data=False, \n",
    "                                            input_folder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                                            patient_number=1, \n",
    "                                            sd_magnitude=5,\n",
    "                                            previous_wave_name=os.path.join(PROJECT_PATH,\"initial_sweep_all_patients_but_1\",\"wave0_patient1_sd_5\"))\n",
    "history_matching.plot_nroy(input_folder=\"patient1_sd_5_all_patients/wave1\", \n",
    "                           wave=wave, \n",
    "                           literature_data=False,\n",
    "                           patient_number=1, \n",
    "                           sd_magnitude=5, \n",
    "                           title = \"Second wave for #1 with SD=5% using data from all previous patients\")\n",
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, \n",
    "                                           output_folder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                                           input_folder=\"patient1_sd_5_all_patients/wave1\",\n",
    "                                           wave_name=\"wave1_patient1_sd_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check how is it going with the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.07, 2.15, 2.66, 2.99, 1.14, 0.93, 1.6, 2.0, 0.43, 0.62, 1.4, 0.22, 0.03]\n",
      "The numerators are [0.49, 37.6, 23.81, 40.25, 1.93, 1.71, 19.92, 1.94, 1.04, 0.41, 8.92, 1.84, 0.26]\n",
      "The denominators are [6.64, 17.49, 8.96, 13.45, 1.68, 1.85, 12.47, 0.97, 2.42, 0.66, 6.38, 8.54, 10.3]\n",
      "The emulated results are [116.0, 141.36, 46.51, 72.54, 22.22, 26.58, 149.33, 10.37, 44.01, 11.57, 77.24, 80.96, 82.17]\n",
      "The emulated SDs are [3.18, 15.02, 8.24, 12.21, 1.17, 1.19, 9.16, 0.75, 1.11, 0.35, 4.7, 7.46, 9.45]\n"
     ]
    }
   ],
   "source": [
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step is used to fail with implausibility >3.2 but now is 2.99 so all good so far. We run the last wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_meshes\n",
    "import preprocess_mesh\n",
    "import ep_simulations\n",
    "import biomarkers\n",
    "import emulators\n",
    "import history_matching\n",
    "import os\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "print(\"Running wave 2...\")\n",
    "\n",
    "generate_meshes.sample_atlas(subfolder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                             csv_filename=\"input_anatomy_training.csv\")\n",
    "preprocess_mesh.biv_setup(subfolder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                          anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                          ep_dat_file=\"input_ep_training.dat\")\n",
    "ep_simulations.run(subfolder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "biomarkers.extract(subfolder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\",\"patient1_sd_5_all_patients/wave2\"])\n",
    "history_matching.save_patient_implausibility(emulators_vector=emulators_vector, \n",
    "                                             input_folder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                                             patient_number=1, sd_magnitude=5)\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, \n",
    "                                            implausibility_threshold=3.,\n",
    "                                            literature_data=False, \n",
    "                                            input_folder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                                            patient_number=1, \n",
    "                                            sd_magnitude=5,\n",
    "                                            previous_wave_name=os.path.join(PROJECT_PATH,\"patient1_sd_5_all_patients\",\"wave1\",\"wave1_patient1_sd_5\"))\n",
    "history_matching.plot_nroy(input_folder=\"patient1_sd_5_all_patients/wave2\", \n",
    "                           wave=wave, \n",
    "                           literature_data=False,\n",
    "                           patient_number=1, \n",
    "                           sd_magnitude=5, \n",
    "                           title = \"Third wave for #1 with SD=5% using data from all previous patients\")\n",
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, \n",
    "                                           output_folder=\"patient1_sd_5_all_patients/wave3\",\n",
    "                                           input_folder=\"patient1_sd_5_all_patients/wave2\",\n",
    "                                           wave_name=\"wave2_patient1_sd_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check with the implausibility of the ground truth points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.09, 2.24, 2.36, 3.35, 1.19, 0.92, 1.71, 2.54, 0.74, 1.15, 1.91, 0.19, 0.0]\n",
      "The numerators are [0.68, 34.23, 23.33, 41.27, 2.01, 1.87, 22.11, 2.57, 1.78, 0.82, 11.7, 1.53, 0.04]\n",
      "The denominators are [7.36, 15.3, 9.9, 12.32, 1.69, 2.02, 12.94, 1.01, 2.39, 0.71, 6.14, 8.06, 9.92]\n",
      "The emulated results are [117.17, 144.73, 46.99, 71.52, 22.14, 26.42, 147.14, 9.74, 44.75, 11.98, 74.46, 81.27, 81.95]\n",
      "The emulated SDs are [4.5, 12.41, 9.25, 10.96, 1.18, 1.44, 9.79, 0.81, 1.05, 0.44, 4.37, 6.91, 9.04]\n"
     ]
    }
   ],
   "source": [
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\",\"patient1_sd_5_all_patients/wave2\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.07, 2.12, 2.66, 3.0, 1.15, 0.92, 1.6, 1.97, 0.43, 0.62, 1.4, 0.21, 0.03]\n",
      "The numerators are [0.49, 37.6, 23.81, 40.25, 1.93, 1.71, 19.92, 1.94, 1.04, 0.41, 8.92, 1.84, 0.26]\n",
      "The denominators are [6.64, 17.72, 8.94, 13.44, 1.67, 1.86, 12.46, 0.98, 2.41, 0.67, 6.39, 8.8, 10.14]\n",
      "The emulated results are [116.0, 141.36, 46.51, 72.54, 22.22, 26.58, 149.33, 10.37, 44.01, 11.57, 77.24, 80.96, 82.17]\n",
      "The emulated SDs are [3.19, 15.3, 8.22, 12.2, 1.16, 1.21, 9.15, 0.77, 1.1, 0.36, 4.72, 7.76, 9.27]\n"
     ]
    }
   ],
   "source": [
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.02, 2.87, 3.27, 3.63, 1.53, 0.55, 2.14, 2.51, 0.5, 0.98, 1.75, 0.75, 1.06]\n",
      "The numerators are [0.11, 41.66, 24.95, 38.65, 2.31, 1.09, 27.85, 2.49, 1.17, 0.74, 9.72, 4.81, 7.66]\n",
      "The denominators are [7.13, 14.53, 7.64, 10.65, 1.51, 1.98, 13.01, 0.99, 2.36, 0.76, 5.54, 6.4, 7.2]\n",
      "The emulated results are [116.6, 137.3, 45.37, 74.14, 21.84, 27.2, 141.4, 9.82, 44.14, 11.9, 76.44, 77.99, 74.25]\n",
      "The emulated SDs are [4.12, 11.44, 6.78, 9.03, 0.91, 1.39, 9.88, 0.78, 0.97, 0.51, 3.48, 4.88, 5.93]\n"
     ]
    }
   ],
   "source": [
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep\",\"patient1_sd_5/wave1\"],\n",
    "                                       patient_number=1, sd_magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try now rerunning it but with SD=10%. Wave 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emulators\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\"])\n",
    "\n",
    "import history_matching\n",
    "\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, implausibility_threshold=3.2,\n",
    "                                                    literature_data=False, input_folder=\"initial_sweep_all_patients_but_1\",\n",
    "                                                    patient_number=1, sd_magnitude=10,\n",
    "                                                    first_time=True)\n",
    "\n",
    "history_matching.plot_nroy(input_folder=\"initial_sweep_all_patients_but_1\", wave=wave, literature_data=False,\n",
    "                                   patient_number=1, sd_magnitude=10,\n",
    "                                   title = \"Initial wave for #1 with SD=10% using data from all previous patients\")\n",
    "\n",
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, output_folder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                                                   input_folder=\"initial_sweep_all_patients_but_1\", wave_name=\"wave0_patient1_sd_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wave 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 196/196 [00:37<00:00,  5.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import generate_meshes\n",
    "import preprocess_mesh\n",
    "import ep_simulations\n",
    "import biomarkers\n",
    "import emulators\n",
    "import history_matching\n",
    "import os\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "print(\"Running wave 1...\")\n",
    "\n",
    "generate_meshes.sample_atlas(subfolder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                             csv_filename=\"input_anatomy_training.csv\")\n",
    "preprocess_mesh.biv_setup(subfolder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                          anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                          ep_dat_file=\"input_ep_training.dat\")\n",
    "ep_simulations.run(subfolder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "biomarkers.extract(subfolder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_10_all_patients/wave1\"])\n",
    "history_matching.save_patient_implausibility(emulators_vector=emulators_vector, \n",
    "                                             input_folder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                                             patient_number=1, sd_magnitude=10)\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, \n",
    "                                            implausibility_threshold=3.2,\n",
    "                                            literature_data=False, \n",
    "                                            input_folder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                                            patient_number=1, \n",
    "                                            sd_magnitude=10,\n",
    "                                            previous_wave_name=os.path.join(PROJECT_PATH,\"initial_sweep_all_patients_but_1\",\"wave0_patient1_sd_10\"))\n",
    "history_matching.plot_nroy(input_folder=\"patient1_sd_10_all_patients/wave1\", \n",
    "                           wave=wave, \n",
    "                           literature_data=False,\n",
    "                           patient_number=1, \n",
    "                           sd_magnitude=10, \n",
    "                           title = \"Second wave for #1 with SD=10% using data from all previous patients\")\n",
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, \n",
    "                                           output_folder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                                           input_folder=\"patient1_sd_10_all_patients/wave1\",\n",
    "                                           wave_name=\"wave1_patient1_sd_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last wave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_meshes\n",
    "import preprocess_mesh\n",
    "import ep_simulations\n",
    "import biomarkers\n",
    "import emulators\n",
    "import history_matching\n",
    "import os\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "print(\"Running wave 2...\")\n",
    "\n",
    "generate_meshes.sample_atlas(subfolder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                             csv_filename=\"input_anatomy_training.csv\")\n",
    "preprocess_mesh.biv_setup(subfolder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                          anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                          ep_dat_file=\"input_ep_training.dat\")\n",
    "ep_simulations.run(subfolder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "biomarkers.extract(subfolder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                   anatomy_csv_file=\"input_anatomy_training.csv\",\n",
    "                   ep_dat_file=\"input_ep_training.dat\")\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_10_all_patients/wave1\",\"patient1_sd_10_all_patients/wave2\"])\n",
    "history_matching.save_patient_implausibility(emulators_vector=emulators_vector, \n",
    "                                             input_folder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                                             patient_number=1, sd_magnitude=10)\n",
    "wave = history_matching.compute_nroy_region(emulators_vector=emulators_vector, \n",
    "                                            implausibility_threshold=3.,\n",
    "                                            literature_data=False, \n",
    "                                            input_folder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                                            patient_number=1, \n",
    "                                            sd_magnitude=10,\n",
    "                                            previous_wave_name=os.path.join(PROJECT_PATH,\"patient1_sd_10_all_patients\",\"wave1\",\"wave1_patient1_sd_10\"))\n",
    "history_matching.plot_nroy(input_folder=\"patient1_sd_10_all_patients/wave2\", \n",
    "                           wave=wave, \n",
    "                           literature_data=False,\n",
    "                           patient_number=1, \n",
    "                           sd_magnitude=10, \n",
    "                           title = \"Third wave for #1 with SD=10% using data from all previous patients\")\n",
    "history_matching.generate_new_training_pts(wave=wave, num_pts=140, \n",
    "                                           output_folder=\"patient1_sd_10_all_patients/wave3\",\n",
    "                                           input_folder=\"patient1_sd_10_all_patients/wave2\",\n",
    "                                           wave_name=\"wave2_patient1_sd_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomarkers are: \n",
      "['LVV' 'RVV' 'LAV' 'RAV' 'LVOTdiam' 'RVOTdiam' 'LVmass' 'LVWT' 'LVEDD'\n",
      " 'SeptumWT' 'RVlongdiam' 'TAT' 'TATLVendo']\n",
      "The implausibilities are [0.05, 0.39, 0.41, 0.89, 0.26, 0.21, 0.33, 0.22, 0.1, 0.61, 0.33, 0.27, 0.14]\n",
      "The numerators are [0.69, 9.79, 4.95, 13.8, 0.73, 0.68, 7.11, 0.34, 0.43, 0.8, 3.23, 3.03, 1.83]\n",
      "The denominators are [14.57, 25.09, 12.01, 15.49, 2.81, 3.25, 21.64, 1.54, 4.47, 1.3, 9.86, 11.39, 13.55]\n",
      "The emulated results are [115.8, 169.17, 65.37, 98.99, 23.42, 27.61, 162.14, 11.97, 42.54, 11.96, 82.93, 85.83, 83.74]\n",
      "The emulated SDs are [8.75, 17.59, 9.74, 10.61, 1.43, 1.59, 13.48, 0.93, 1.23, 0.67, 4.79, 7.82, 10.79]\n"
     ]
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.print_patient_implausibility_terms(emulators_folders=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_10_all_patients/wave1\",\"patient1_sd_10_all_patients/wave2\"],\n",
    "                                       patient_number=1, sd_magnitude=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.97% of the space of patient1_sd_5_all_patients matches that of patient1_sd_10_all_patients\n",
      "75.97% of the space of patient1_sd_10_all_patients matches that of patient1_sd_5_all_patients\n",
      "98.15% of the NROY of patient1_sd_5_all_patients matches that of patient1_sd_10_all_patients\n",
      "11.78% of the NROY of patient1_sd_10_all_patients matches that of patient1_sd_5_all_patients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[75.97, 75.97, 98.15, 11.78]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import postprocessing\n",
    "\n",
    "postprocessing.compare_nroy_two_cases(folder_a=\"patient1_sd_5_all_patients\", wave_a_number=2, wave_a_name=\"wave2_patient1_sd_5\",\n",
    "                           emulators_a=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_5_all_patients/wave1\",\"patient1_sd_5_all_patients/wave2\"],\n",
    "                           biomarkers_patient_number_a=1, sd_magnitude_a=5,\n",
    "                           folder_b=\"patient1_sd_10_all_patients\", wave_b_number=2, wave_b_name=\"wave2_patient1_sd_10\",\n",
    "                           emulators_b=[\"initial_sweep_all_patients_but_1\",\"patient1_sd_10_all_patients/wave1\",\"patient1_sd_10_all_patients/wave2\"],\n",
    "                           biomarkers_patient_number_b=1, sd_magnitude_b=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-hm",
   "language": "python",
   "name": "python3.8-hm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
