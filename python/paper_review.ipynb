{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing emulators with one of Marina's cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "mesh_path='/media/crg17/SeagateExpansionDrive'\n",
    "mesh_name = \"HF01\"\n",
    "mesh_dir = os.path.join(mesh_path, mesh_name)\n",
    "\n",
    "pathlib.Path(mesh_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.system(\"meshtool convert -imsh=\" + os.path.join(mesh_dir, mesh_name + \".vtk\") +\n",
    "      \" -omsh=\" + os.path.join(mesh_dir, mesh_name) +\n",
    "      \" -ifmt=vtk -ofmt=carp_txt\")\n",
    "\n",
    "os.rename(os.path.join(mesh_dir, mesh_name) + \".pts\", os.path.join(mesh_dir, mesh_name) + \"_mm.pts\")\n",
    "\n",
    "os.system(os.path.join(\"/home\", \"common\", \"cm2carp\", \"bin\", \"return_carp2original_coord.pl \") +\n",
    "          os.path.join(mesh_dir, mesh_name) + \"_mm.pts 1000 0 0 0 > \" +\n",
    "          os.path.join(mesh_dir, mesh_name) + \"_um.pts\")\n",
    "\n",
    "shutil.copy(os.path.join(mesh_dir, mesh_name) + \"_um.pts\",\n",
    "            os.path.join(mesh_dir, mesh_name) + \".pts\")\n",
    "\n",
    "shutil.copy(os.path.join(mesh_dir, mesh_name + \".elem\"),\n",
    "            os.path.join(mesh_dir, mesh_name + \"_default.elem\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import files_manipulations\n",
    "import os\n",
    "import shutil \n",
    "\n",
    "mesh_path='/media/crg17/SeagateExpansionDrive'\n",
    "mesh_name = \"HF01\"\n",
    "mesh_dir = os.path.join(mesh_path, mesh_name)\n",
    "\n",
    "shutil.copy(os.path.join(mesh_dir, mesh_name + \"_default.elem\"),\n",
    "            os.path.join(mesh_dir, mesh_name + \"_original_tags.elem\"))\n",
    "\n",
    "marina_tags = files_manipulations.elem.read(os.path.join(mesh_dir, mesh_name + \"_original_tags.elem\"))\n",
    "\n",
    "marina_tags.change_tags(old_tags_list = [7, 8, 9, 10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],\n",
    "                        new_tags_list = [18,21,20,19,22,23,24,7,8,9,10,11,12,13,14,15,16,17])\n",
    "\n",
    "marina_tags.write(os.path.join(mesh_dir, mesh_name + \".elem\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.extract_ldrb_biv(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.extract_mvtv_base(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.close_lv_endo(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"LVV.dat\"),\n",
    "           [round(np.abs(sum(files_manipulations.area_or_vol_surface(\n",
    "               pts_file=files_manipulations.pts.read(os.path.join(PROJECT_PATH, \"HF01\", \"HF01.pts\")),\n",
    "               surf_file=files_manipulations.surf.read(os.path.join(PROJECT_PATH, \"HF01\",\"lvendo_closed.surf\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=True,with_area=False)) * 1e-12), 2)],fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.close_rv_endo(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"RVV.dat\"),\n",
    "           [round(np.abs(sum(files_manipulations.area_or_vol_surface(\n",
    "               pts_file=files_manipulations.pts.read(os.path.join(PROJECT_PATH, \"HF01\", \"HF01.pts\")),\n",
    "               surf_file=files_manipulations.surf.read(os.path.join(PROJECT_PATH, \"HF01\",\"rvendo_closed.surf\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=True,with_area=False)) * 1e-12), 2)],fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.close_LA_endo(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"LAV.dat\"),\n",
    "           [round(np.abs(sum(files_manipulations.area_or_vol_surface(\n",
    "               pts_file=files_manipulations.pts.read(os.path.join(PROJECT_PATH, \"HF01\", \"laendo_closed.pts\")),\n",
    "               surf_file=files_manipulations.surf.read(os.path.join(PROJECT_PATH, \"HF01\",\"laendo_closed.elem\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=True,with_area=False)) * 1e-12), 2)],fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_mesh\n",
    "\n",
    "prepare_mesh.close_RA_endo(fourch_name=\"HF01\",subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"RAV.dat\"),\n",
    "           [round(np.abs(sum(files_manipulations.area_or_vol_surface(\n",
    "               pts_file=files_manipulations.pts.read(os.path.join(PROJECT_PATH, \"HF01\", \"raendo_closed.pts\")),\n",
    "               surf_file=files_manipulations.surf.read(os.path.join(PROJECT_PATH, \"HF01\",\"raendo_closed.elem\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=True,with_area=False)) * 1e-12), 2)],fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"LVOTdiam.dat\"),\n",
    "                   [round(2 * np.sqrt((sum(files_manipulations.area_or_vol_surface(\n",
    "                                pts_file=files_manipulations.pts.read(os.path.join(\n",
    "                                    PROJECT_PATH, \"HF01\",\n",
    "                                    \"av_mapped.pts\")\n",
    "                                ),surf_file=files_manipulations.surf.read(\n",
    "                                    os.path.join(PROJECT_PATH, \"HF01\",\n",
    "                                                 \"av_ao.surf\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=False,with_area=True)) * 1e-6) / np.pi), 2)], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import files_manipulations\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"RVOTdiam.dat\"),\n",
    "                   [round(2 * np.sqrt((sum(files_manipulations.area_or_vol_surface(\n",
    "                                pts_file=files_manipulations.pts.read(os.path.join(\n",
    "                                    PROJECT_PATH, \"HF01\",\n",
    "                                    \"pv_mapped.pts\")\n",
    "                                ),surf_file=files_manipulations.surf.read(\n",
    "                                    os.path.join(PROJECT_PATH, \"HF01\",\n",
    "                                                 \"pv_pa.surf\"),mesh_from=\"HF01\"),\n",
    "                                with_vol=False,with_area=True)) * 1e-6) / np.pi), 2)], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run outside of a jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import UVC\n",
    "\n",
    "UVC.create(fourch_name=\"HF01\", base=\"mvtv\", subfolder=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /media/crg17/SeagateExpansionDrive/HF01/biv/biv.elem...\n",
      "File read.\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from global_variables_config import *\n",
    "\n",
    "import ep_simulations\n",
    "import files_manipulations\n",
    "import prepare_mesh\n",
    "\n",
    "\n",
    "return_LVmass = False\n",
    "return_ISWT = False\n",
    "return_WT = False\n",
    "return_EDD = False\n",
    "return_RVlongdiam = False\n",
    "if not os.path.isfile(\n",
    "        os.path.join(PROJECT_PATH, \"HF01\", \"LVmass.dat\")):\n",
    "    return_LVmass = True\n",
    "if not os.path.isfile(\n",
    "        os.path.join(PROJECT_PATH, \"HF01\", \"LVWT.dat\")):\n",
    "    return_WT = True\n",
    "if not os.path.isfile(\n",
    "        os.path.join(PROJECT_PATH, \"HF01\", \"LVEDD.dat\")):\n",
    "    return_EDD = True\n",
    "if not os.path.isfile(\n",
    "        os.path.join(PROJECT_PATH, \"HF01\", \"SeptumWT.dat\")):\n",
    "    return_ISWT = True\n",
    "if not os.path.isfile(\n",
    "        os.path.join(PROJECT_PATH, \"HF01\", \"RVlongdiam.dat\")):\n",
    "    return_RVlongdiam = True\n",
    "path2fourch = os.path.join(PROJECT_PATH, \"HF01\")\n",
    "biv_path = os.path.join(path2fourch, \"biv\")\n",
    "heart_name = \"HF01\"\n",
    "if return_RVlongdiam:\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH, \"HF01\", \"pv_pa.surf\")):\n",
    "        prepare_mesh.close_rv_endo(fourch_name=\"HF01\", subfolder=\".\")\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH, \"HF01\", \"tv_rv.pts\")):\n",
    "        prepare_mesh.close_RA_endo(fourch_name=\"HF01\", subfolder=\".\")\n",
    "\n",
    "    rvendo_closed = files_manipulations.pts.read(os.path.join(path2fourch, heart_name + \".pts\"))\n",
    "\n",
    "    tvrv_pts = files_manipulations.pts.read(os.path.join(path2fourch, \"tv_mapped.pts\"))\n",
    "\n",
    "    tvrv_surf = files_manipulations.surf.read(os.path.join(biv_path, \"..\", \"tv_ra.surf\"), heart_name)\n",
    "\n",
    "    area_array = files_manipulations.area_or_vol_surface(pts_file=tvrv_pts,\n",
    "                                                         surf_file=tvrv_surf, with_vol=False,\n",
    "                                                         with_area=True)\n",
    "    tvrv_area = sum(area_array) * 1e-6  # In mm2\n",
    "\n",
    "    RVbasaldiam = 2 * np.sqrt(tvrv_area / np.pi)\n",
    "\n",
    "    num_pts = tvrv_pts.size\n",
    "\n",
    "    sum_x = np.sum(tvrv_pts.p1)\n",
    "    sum_y = np.sum(tvrv_pts.p2)\n",
    "    sum_z = np.sum(tvrv_pts.p3)\n",
    "\n",
    "    centroid = np.array([sum_x / num_pts, sum_y / num_pts, sum_z / num_pts])\n",
    "\n",
    "    dist_vec = np.zeros(rvendo_closed.size)\n",
    "\n",
    "    for j in range(len(dist_vec)):\n",
    "        new_point = np.array([rvendo_closed.p1[j], rvendo_closed.p2[j], rvendo_closed.p3[j]])\n",
    "        dist_vec[j] = np.linalg.norm(centroid - new_point)\n",
    "\n",
    "    RVlongdiam_centroid = max(dist_vec) * 1e-3\n",
    "\n",
    "    RVlongdiam = np.sqrt((RVbasaldiam / 2.) ** 2 + RVlongdiam_centroid ** 2)\n",
    "\n",
    "    np.savetxt(os.path.join(PROJECT_PATH, \"HF01\", \"RVlongdiam.dat\"),\n",
    "               [round(RVlongdiam, 2)], fmt=\"%s\")\n",
    "\n",
    "if return_ISWT or return_WT or return_EDD or return_LVmass:\n",
    "    path2UVC = os.path.join(biv_path, \"UVC_mvtv\", \"UVC\")\n",
    "    biv_pts = files_manipulations.pts.read(os.path.join(biv_path, \"biv.pts\"))\n",
    "    UVC_Z_elem = np.genfromtxt(os.path.join(path2UVC, \"COORDS_Z_elem_scaled.dat\"), dtype=float)\n",
    "    UVC_Z = np.genfromtxt(os.path.join(path2UVC, \"COORDS_Z.dat\"), dtype=float)\n",
    "\n",
    "    if return_LVmass:\n",
    "        biv_elem = files_manipulations.elem.read(os.path.join(biv_path, \"biv.elem\"))\n",
    "        UVC_V_elem = np.genfromtxt(os.path.join(path2UVC, \"COORDS_V_elem.dat\"), dtype=float)\n",
    "\n",
    "        def six_vol_element_cm3(i):\n",
    "            result = np.linalg.det(np.array([np.array(\n",
    "                [1e-4 * biv_pts.p1[biv_elem.i1[i]], 1e-4 * biv_pts.p2[biv_elem.i1[i]],\n",
    "                 1e-4 * biv_pts.p3[biv_elem.i1[i]], 1.], dtype=float),\n",
    "                                             np.array([1e-4 * biv_pts.p1[biv_elem.i2[i]],\n",
    "                                                       1e-4 * biv_pts.p2[biv_elem.i2[i]],\n",
    "                                                       1e-4 * biv_pts.p3[biv_elem.i2[i]], 1.], dtype=float),\n",
    "                                             np.array([1e-4 * biv_pts.p1[biv_elem.i3[i]],\n",
    "                                                       1e-4 * biv_pts.p2[biv_elem.i3[i]],\n",
    "                                                       1e-4 * biv_pts.p3[biv_elem.i3[i]], 1.], dtype=float),\n",
    "                                             np.array([1e-4 * biv_pts.p1[biv_elem.i4[i]],\n",
    "                                                       1e-4 * biv_pts.p2[biv_elem.i4[i]],\n",
    "                                                       1e-4 * biv_pts.p3[biv_elem.i4[i]], 1.], dtype=float)\n",
    "                                             ], dtype=float)\n",
    "                                   )\n",
    "            return np.abs(result)\n",
    "\n",
    "        LV_mass_idx = np.intersect1d(np.where(UVC_Z_elem < 0.9), np.where(UVC_V_elem < 0))\n",
    "\n",
    "        six_vol_LV = []\n",
    "        six_vol_LV.append(Parallel(n_jobs=20)(delayed(six_vol_element_cm3)(i) for i in range(len(LV_mass_idx))))\n",
    "\n",
    "        LV_mass = 1.05 * sum(six_vol_LV[0]) / 6.\n",
    "\n",
    "        np.savetxt(\n",
    "            os.path.join(PROJECT_PATH, \"HF01\", \"LVmass.dat\"),\n",
    "            [round(LV_mass, 2)], fmt=\"%s\")\n",
    "\n",
    "    if return_ISWT or return_WT or return_EDD:\n",
    "        septum_idx = files_manipulations.vtx.read(os.path.join(biv_path, \"biv.rvsept.surf.vtx\"), \"biv\")\n",
    "        lvendo_idx = files_manipulations.vtx.read(os.path.join(biv_path, \"biv.lvendo.surf.vtx\"), \"biv\")\n",
    "        UVC_PHI = np.genfromtxt(os.path.join(path2UVC, \"COORDS_PHI.dat\"), dtype=float)\n",
    "        septum_Z = UVC_Z[septum_idx.indices]\n",
    "        lvendo_Z = UVC_Z[lvendo_idx.indices]\n",
    "        septum_band_idx = septum_idx.indices[np.intersect1d(np.where(septum_Z > 0.6),\n",
    "                                                            np.where(septum_Z < 0.9)\n",
    "                                                            )\n",
    "        ]\n",
    "        lvendo_band_idx = lvendo_idx.indices[np.intersect1d(np.where(lvendo_Z > 0.6),\n",
    "                                                            np.where(lvendo_Z < 0.9)\n",
    "                                                            )\n",
    "        ]\n",
    "        septum_band_PHI = UVC_PHI[septum_band_idx]\n",
    "        lvendo_band_PHI = UVC_PHI[lvendo_band_idx]\n",
    "\n",
    "        midpoint_PHI = (max(septum_band_PHI) + min(septum_band_PHI)) / 2.\n",
    "        bandwith_PHI = max(septum_band_PHI) - min(septum_band_PHI)\n",
    "        min_PHI = midpoint_PHI - bandwith_PHI / 6.\n",
    "        max_PHI = midpoint_PHI + bandwith_PHI / 6.\n",
    "\n",
    "    if return_ISWT or return_EDD:\n",
    "        lvendo_septum_ROI_idx = lvendo_band_idx[np.intersect1d(np.where(lvendo_band_PHI > min_PHI),\n",
    "                                                               np.where(lvendo_band_PHI < max_PHI)\n",
    "                                                               )\n",
    "        ]\n",
    "        lvendo_septum_ROI_pts = biv_pts.extract(files_manipulations.vtx(lvendo_septum_ROI_idx, \"biv\"))\n",
    "\n",
    "        if return_ISWT:\n",
    "            septum_ROI_idx = septum_band_idx[np.intersect1d(np.where(septum_band_PHI > min_PHI),\n",
    "                                                            np.where(septum_band_PHI < max_PHI)\n",
    "                                                            )\n",
    "            ]\n",
    "            septum_ROI_pts = biv_pts.extract(files_manipulations.vtx(septum_ROI_idx, \"biv\"))\n",
    "\n",
    "            def dist_from_septum_to_lvendo_septum(i):\n",
    "                return lvendo_septum_ROI_pts.min_dist(\n",
    "                    np.array([septum_ROI_pts.p1[i], septum_ROI_pts.p2[i], septum_ROI_pts.p3[i]]))\n",
    "\n",
    "            septum_ROI_thickness = []\n",
    "            septum_ROI_thickness.append(Parallel(n_jobs=20)(\n",
    "                delayed(dist_from_septum_to_lvendo_septum)(i) for i in range(septum_ROI_pts.size)))\n",
    "\n",
    "            # Itraventricular septal wall thickness, in mm\n",
    "            ISWT = 1e-3 * np.median(septum_ROI_thickness)\n",
    "\n",
    "            np.savetxt(\n",
    "                os.path.join(PROJECT_PATH, \"HF01\", \"SeptumWT.dat\"),\n",
    "                [round(ISWT, 2)], fmt=\"%s\")\n",
    "\n",
    "\n",
    "    if return_EDD or return_WT:\n",
    "        # Wall thickness as the opposite side of the septum\n",
    "        if min_PHI <= 0:\n",
    "            wall_min_PHI = min_PHI + np.pi\n",
    "        else:\n",
    "            wall_min_PHI = min_PHI - np.pi\n",
    "\n",
    "        if max_PHI <= 0:\n",
    "            wall_max_PHI = max_PHI + np.pi\n",
    "        else:\n",
    "            wall_max_PHI = max_PHI - np.pi\n",
    "\n",
    "        if wall_max_PHI * wall_min_PHI > 0:\n",
    "            lvendo_wall_ROI_idx = lvendo_band_idx[np.intersect1d(np.where(lvendo_band_PHI > wall_min_PHI)[0],\n",
    "                                                                 np.where(lvendo_band_PHI < wall_max_PHI)[0]\n",
    "                                                                 )\n",
    "            ]\n",
    "        else:\n",
    "\n",
    "            upper_dir = np.where(lvendo_band_PHI > wall_min_PHI)\n",
    "            lower_dir = np.where(lvendo_band_PHI < wall_max_PHI)\n",
    "\n",
    "            # Otherwise is an array of arrays of arrays\n",
    "            all_indices_flat = [subitem for sublist in [lower_dir, upper_dir] for item in sublist for subitem in\n",
    "                                item]\n",
    "\n",
    "            lvendo_wall_ROI_idx = lvendo_band_idx[np.unique(all_indices_flat)]\n",
    "\n",
    "        lvendo_wall_ROI_pts = biv_pts.extract(files_manipulations.vtx(lvendo_wall_ROI_idx, \"biv\"))\n",
    "\n",
    "        if return_WT:\n",
    "            bivepi_idx = files_manipulations.vtx.read(os.path.join(biv_path, \"biv.epi.surf.vtx\"),\n",
    "                                                      \"biv\")\n",
    "            bivepi_Z = UVC_Z[bivepi_idx.indices]\n",
    "            bivepi_band_idx = bivepi_idx.indices[np.intersect1d(np.where(bivepi_Z > 0.6),\n",
    "                                                                np.where(bivepi_Z < 0.9)\n",
    "                                                                )\n",
    "            ]\n",
    "            bivepi_band_PHI = UVC_PHI[bivepi_band_idx]\n",
    "\n",
    "            if wall_max_PHI * wall_min_PHI > 0:\n",
    "                lvepi_ROI_idx = bivepi_band_idx[np.intersect1d(np.where(bivepi_band_PHI > wall_min_PHI),\n",
    "                                                               np.where(bivepi_band_PHI < wall_max_PHI)\n",
    "                                                               )\n",
    "                ]\n",
    "            else:\n",
    "                upper_dir = np.where(bivepi_band_PHI > wall_min_PHI)\n",
    "                lower_dir = np.where(bivepi_band_PHI < wall_max_PHI)\n",
    "\n",
    "                # Otherwise is an array of arrays of arrays\n",
    "                all_indices_flat = [subitem for sublist in [lower_dir, upper_dir] for item in sublist for subitem in\n",
    "                                    item]\n",
    "\n",
    "                lvepi_ROI_idx = bivepi_band_idx[np.unique(all_indices_flat)]\n",
    "\n",
    "            lvepi_ROI_pts = biv_pts.extract(files_manipulations.vtx(lvepi_ROI_idx, \"biv\"))\n",
    "\n",
    "            def dist_from_lvepi_to_lvendo_wall(i):\n",
    "                return lvendo_wall_ROI_pts.min_dist(\n",
    "                    np.array([lvepi_ROI_pts.p1[i], lvepi_ROI_pts.p2[i], lvepi_ROI_pts.p3[i]]))\n",
    "\n",
    "            wall_ROI_thickness = []\n",
    "            wall_ROI_thickness.append(\n",
    "                Parallel(n_jobs=20)(delayed(dist_from_lvepi_to_lvendo_wall)(i) for i in range(lvepi_ROI_pts.size)))\n",
    "\n",
    "            # Wall thickness, in mm\n",
    "            WT = 1e-3 * np.median(wall_ROI_thickness)\n",
    "\n",
    "            np.savetxt(\n",
    "                os.path.join(PROJECT_PATH, \"HF01\", \"LVWT.dat\"),\n",
    "                [round(WT, 2)], fmt=\"%s\")\n",
    "\n",
    "\n",
    "        if return_EDD:\n",
    "            def dist_from_lvendo_septum_to_lvendo_wall(i):\n",
    "                return lvendo_wall_ROI_pts.min_dist(np.array(\n",
    "                    [lvendo_septum_ROI_pts.p1[i], lvendo_septum_ROI_pts.p2[i], lvendo_septum_ROI_pts.p3[i]]))\n",
    "\n",
    "            ED_dimension_ROI = []\n",
    "\n",
    "            ED_dimension_ROI.append(Parallel(n_jobs=20)(\n",
    "                delayed(dist_from_lvendo_septum_to_lvendo_wall)(i) for i in range(lvendo_septum_ROI_pts.size)))\n",
    "\n",
    "            # End-diastolic dimension, mm\n",
    "            EDD = 1e-3 * np.median(ED_dimension_ROI)\n",
    "\n",
    "            np.savetxt(\n",
    "                os.path.join(PROJECT_PATH, \"HF01\", \"LVEDD.dat\"),\n",
    "                [round(EDD, 2)], fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running wave 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -1.2884\n",
      "Weights: tensor([-0.2605,  1.8049,  1.2967,  1.1255, -0.1742,  0.0585, -0.1446, -0.1093,\n",
      "        -1.3067, -0.0134, -0.0179, -0.0129,  0.0096,  0.0158])\n",
      "Outputscale: 0.0791\n",
      "Lengthscales: tensor([1.0814, 0.6945, 1.1772, 1.4223, 1.6394, 5.3571, 2.0625, 3.3446, 1.2779,\n",
      "        8.9920, 6.4037, 7.6805, 7.9630, 4.2153])\n",
      "Likelihood noise: 0.0002\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -1.8116\n",
      "Weights: tensor([    -0.5239,      2.3993,      1.2159,      1.0373,     -0.2163,\n",
      "             0.0834,      0.2197,      0.0647,      0.2654,      0.0058,\n",
      "             0.0232,     -0.0001,     -0.0155,     -0.0043])\n",
      "Outputscale: 0.1002\n",
      "Lengthscales: tensor([ 0.8662,  0.9455,  1.1383,  1.3524,  2.4961,  2.4609,  3.2550,  2.8596,\n",
      "         2.8902,  8.7069,  9.9852, 10.6267,  3.8838,  7.9730])\n",
      "Likelihood noise: 0.0001\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: 1.0687\n",
      "Weights: tensor([ 1.0662,  1.2239,  0.9897,  0.0373,  0.3511, -0.2399,  0.2294, -0.1901,\n",
      "        -0.4688,  0.0169, -0.0443, -0.0502,  0.0072, -0.0064])\n",
      "Outputscale: 1.5851\n",
      "Lengthscales: tensor([ 0.7521,  1.1042,  1.6785,  2.7774,  2.6557,  1.7433,  2.2158,  2.9555,\n",
      "         1.8527,  4.9250,  5.9477,  9.3976, 11.7315, 10.0018])\n",
      "Likelihood noise: 0.0003\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: 0.9075\n",
      "Weights: tensor([-0.7131,  1.7040,  1.2763, -0.2735,  0.5862, -0.4639, -0.5882, -0.1862,\n",
      "         0.2683,  0.0055,  0.0407, -0.0350,  0.0527,  0.0318])\n",
      "Outputscale: 0.6225\n",
      "Lengthscales: tensor([ 0.8452,  1.2584,  1.3190,  2.1786,  2.3548,  2.4179,  1.8775,  2.8930,\n",
      "         2.0991, 10.1086,  6.5312,  8.9847,  6.7141,  4.7555])\n",
      "Likelihood noise: 0.0003\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -1.8230\n",
      "Weights: tensor([ 0.6569,  2.3439,  1.4987, -0.4777,  0.8220, -0.3386,  0.3522, -0.6527,\n",
      "        -0.5103,  0.0121, -0.0267, -0.0141,  0.0118, -0.0032])\n",
      "Outputscale: 0.2464\n",
      "Lengthscales: tensor([0.4194, 0.6898, 0.9091, 1.3045, 1.8494, 1.6940, 1.6272, 2.3499, 0.8699,\n",
      "        3.9061, 4.1934, 5.0727, 6.3269, 7.8100])\n",
      "Likelihood noise: 0.0006\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: 1.9801\n",
      "Weights: tensor([    -1.3425,      0.4525,     -0.0844,      0.4658,     -0.3668,\n",
      "             0.0013,     -0.4760,     -0.1909,     -0.2249,      0.0464,\n",
      "            -0.0242,      0.0148,     -0.0406,     -0.0498])\n",
      "Outputscale: 0.6154\n",
      "Lengthscales: tensor([0.3640, 0.6954, 0.5925, 1.0862, 1.3559, 1.5753, 1.5468, 0.9269, 2.2856,\n",
      "        8.8650, 5.8255, 3.8898, 7.1764, 6.4447])\n",
      "Likelihood noise: 0.0007\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -0.3359\n",
      "Weights: tensor([-0.6878,  2.9246,  1.7596,  0.3128, -0.3448,  0.2881, -0.6415,  0.0451,\n",
      "         0.6847,  0.0066,  0.0096,  0.0560, -0.0118,  0.0169])\n",
      "Outputscale: 0.4061\n",
      "Lengthscales: tensor([ 1.4862,  1.0675,  1.6879,  2.3461,  2.0585,  4.5678,  2.5882,  3.0178,\n",
      "         2.6130, 10.1027,  5.2963,  4.4471,  6.1414, 11.6840])\n",
      "Likelihood noise: 0.0001\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -1.0037\n",
      "Weights: tensor([-0.4735,  2.0477,  1.1109, -0.2808, -0.1004,  0.3350, -0.4708, -0.2753,\n",
      "         1.7285,  0.0049,  0.0164,  0.0160,  0.0169,  0.0533])\n",
      "Outputscale: 0.4643\n",
      "Lengthscales: tensor([0.7668, 0.6834, 1.3471, 1.3697, 1.0941, 1.6585, 1.4423, 2.7331, 2.5654,\n",
      "        6.2777, 3.6174, 7.1346, 4.0577, 2.8194])\n",
      "Likelihood noise: 0.0021\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -0.4683\n",
      "Weights: tensor([-0.8049,  0.8222,  1.4110,  1.7479, -1.0051, -0.2513, -0.7861,  0.6569,\n",
      "        -1.7375, -0.0138, -0.0638,  0.0245, -0.0194,  0.0054])\n",
      "Outputscale: 0.1578\n",
      "Lengthscales: tensor([0.5837, 0.6514, 0.8219, 0.8385, 0.5494, 1.0013, 0.8522, 0.9588, 1.9370,\n",
      "        3.1829, 7.2610, 9.7100, 3.0103, 9.7572])\n",
      "Likelihood noise: 0.0049\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -0.9384\n",
      "Weights: tensor([ 0.0868,  2.0896,  0.6567, -0.0576, -0.9726,  0.1013, -1.6399,  0.1811,\n",
      "         1.6469,  0.0579, -0.0168,  0.0313,  0.0076, -0.0094])\n",
      "Outputscale: 0.0593\n",
      "Lengthscales: tensor([0.5987, 0.5930, 1.0780, 1.3380, 0.8248, 1.2279, 0.8330, 1.0573, 0.8593,\n",
      "        3.9193, 6.4961, 2.7805, 3.3268, 2.3924])\n",
      "Likelihood noise: 0.0028\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: -1.7231\n",
      "Weights: tensor([     0.0537,      2.1672,      1.6690,      0.3825,      0.4245,\n",
      "            -0.0732,      0.4610,      0.2907,      0.2807,      0.0224,\n",
      "             0.0006,      0.0047,     -0.0300,     -0.0364])\n",
      "Outputscale: 0.2869\n",
      "Lengthscales: tensor([0.4658, 0.7301, 0.8340, 0.4773, 1.5377, 0.9881, 0.7189, 1.8519, 1.4618,\n",
      "        4.4471, 6.7462, 7.9439, 4.6320, 3.2964])\n",
      "Likelihood noise: 0.0044\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: 2.6272\n",
      "Weights: tensor([-0.1375,  0.8826,  0.6028,  0.1578,  0.2334,  0.0889,  0.0175, -0.0414,\n",
      "         0.1041, -0.5274, -2.1717, -1.0899, -1.1100, -1.0401])\n",
      "Outputscale: 0.1799\n",
      "Lengthscales: tensor([1.5464, 0.8550, 4.5053, 2.2611, 3.0113, 2.4264, 3.1109, 2.6692, 1.1663,\n",
      "        0.8268, 0.3011, 1.3493, 1.0148, 0.4092])\n",
      "Likelihood noise: 0.0094\n",
      "\n",
      "Loading emulator...\n",
      "\n",
      "Done. The emulator hyperparameters are:\n",
      "\n",
      "Bias: 2.3689\n",
      "Weights: tensor([     0.1908,      0.4040,      0.4476,     -0.1280,      0.1692,\n",
      "             0.0013,      0.1738,     -0.0750,     -0.1503,     -0.4834,\n",
      "            -2.0465,     -0.7654,     -0.5468,     -1.1748])\n",
      "Outputscale: 0.1981\n",
      "Lengthscales: tensor([1.8329, 1.7291, 3.2656, 1.7287, 5.1954, 5.8553, 4.9514, 1.8877, 1.3050,\n",
      "        1.1126, 0.1791, 1.3877, 1.1061, 0.4222])\n",
      "Likelihood noise: 0.0172\n",
      "Computing implausibility of NROY...\n",
      "Finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough points for simulations! n_simuls must be strictly less than W.NIMP.shape[0] - 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 147>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m wave\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(PROJECT_PATH, input_folder, wave_name)) \u001b[38;5;66;03m# Just in case there are not enough points in the NROY\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_pts \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 155\u001b[0m     training_pts \u001b[38;5;241m=\u001b[39m \u001b[43mwave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_pts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     training_pts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/KCL_projects/fitting/python/Historia/Historia/history/hm.py:110\u001b[0m, in \u001b[0;36mWave.get_points\u001b[0;34m(self, n_simuls)\u001b[0m\n\u001b[1;32m    108\u001b[0m NROY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNIMP)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_simuls \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m NROY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough points for simulations! n_simuls must be strictly less than W.NIMP.shape[0] - 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     SIMULS \u001b[38;5;241m=\u001b[39m dp\u001b[38;5;241m.\u001b[39msubset\u001b[38;5;241m.\u001b[39mpsa_select(NROY, n_simuls)\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough points for simulations! n_simuls must be strictly less than W.NIMP.shape[0] - 1."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import diversipy\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import Historia\n",
    "\n",
    "import postprocessing\n",
    "import emulators\n",
    "\n",
    "\n",
    "implausibility_threshold=3.2\n",
    "literature_data=False\n",
    "input_folder=\"initial_sweep\"\n",
    "patient_number=1\n",
    "sd_magnitude=10\n",
    "first_time=True\n",
    "sampling_pts_lhd=1e6\n",
    "previous_wave_name = None\n",
    "\n",
    "pathlib.Path(os.path.join(PROJECT_PATH, input_folder)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emulators_vector = emulators.train(folders=[\"initial_sweep\"])\n",
    "\n",
    "SEED = 2\n",
    "# ----------------------------------------------------------------\n",
    "# Make the code reproducible\n",
    "np.random.seed(SEED)\n",
    "\n",
    "param_ranges_lower_anatomy = np.loadtxt(os.path.join(PROJECT_PATH, \"anatomy_input_range_lower.dat\"),\n",
    "                                        dtype=float)\n",
    "param_ranges_upper_anatomy = np.loadtxt(os.path.join(PROJECT_PATH, \"anatomy_input_range_upper.dat\"),\n",
    "                                        dtype=float)\n",
    "\n",
    "param_ranges_lower_ep = np.loadtxt(os.path.join(PROJECT_PATH, \"EP_input_range_lower.dat\"), dtype=float)\n",
    "param_ranges_upper_ep = np.loadtxt(os.path.join(PROJECT_PATH, \"EP_input_range_upper.dat\"), dtype=float)\n",
    "\n",
    "param_ranges_lower = np.append(param_ranges_lower_anatomy, param_ranges_lower_ep)\n",
    "param_ranges_upper = np.append(param_ranges_upper_anatomy, param_ranges_upper_ep)\n",
    "\n",
    "param_ranges = np.column_stack((param_ranges_lower, param_ranges_upper))\n",
    "\n",
    "patients_simulation_output = np.loadtxt(open(os.path.join(PROJECT_PATH, \"anatomy_EP_HF_patients.csv\"), \"rb\"),\n",
    "                                           delimiter=\",\", skiprows=1)\n",
    "exp_mean = patients_simulation_output[patient_number-1,:-1]\n",
    "exp_std = sd_magnitude/100.*exp_mean\n",
    "exp_var = np.power(exp_std, 2)\n",
    "\n",
    "emulators_vector = emulators_vector[:-1]\n",
    "\n",
    "if first_time:\n",
    "    wave = Historia.history.hm.Wave(emulator=emulators_vector,\n",
    "                                    Itrain=param_ranges,\n",
    "                                    cutoff=implausibility_threshold,\n",
    "                                    maxno=1,\n",
    "                                    mean=exp_mean,\n",
    "                                    var=exp_var)\n",
    "else:\n",
    "    wave = Historia.history.hm.Wave()\n",
    "    wave.load(previous_wave_name)\n",
    "    wave.emulator = emulators_vector\n",
    "    wave.Itrain = param_ranges\n",
    "    wave.cutoff = implausibility_threshold\n",
    "    wave.maxno = 1\n",
    "    wave.mean = exp_mean\n",
    "    wave.var = exp_var\n",
    "\n",
    "if sampling_pts_lhd == 0:\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH,input_folder, \"points_to_emulate.dat\")):\n",
    "        if first_time:\n",
    "            points_to_emulate = Historia.shared.design_utils.lhd(param_ranges, int(1e5), SEED)\n",
    "        else:\n",
    "            points_to_emulate = add_points_to_nroy(input_wave=wave, param_ranges=param_ranges, n_pts=int(1e5),\n",
    "                                                   scale=0.1)\n",
    "        np.savetxt(os.path.join(PROJECT_PATH,input_folder, \"points_to_emulate.dat\"), points_to_emulate, fmt=\"%.2f\")\n",
    "    else:\n",
    "        points_to_emulate = np.loadtxt(os.path.join(PROJECT_PATH,input_folder, \"points_to_emulate.dat\"), dtype=float)\n",
    "\n",
    "else:\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH,input_folder,\"points_to_emulate_lhd_\" +\n",
    "                                                                 str(int(sampling_pts_lhd)) + \".dat\")):\n",
    "        points_to_emulate = Historia.shared.design_utils.lhd(param_ranges,int(sampling_pts_lhd),SEED)\n",
    "        np.savetxt(os.path.join(PROJECT_PATH,input_folder,\"points_to_emulate_lhd_\" +\n",
    "                                str(int(sampling_pts_lhd)) + \".dat\"), points_to_emulate, fmt=\"%.2f\")\n",
    "    else:\n",
    "        points_to_emulate = np.loadtxt(os.path.join(PROJECT_PATH,input_folder,\"points_to_emulate_lhd_\" +\n",
    "                                str(int(sampling_pts_lhd)) + \".dat\"), dtype=float)\n",
    "# ============= We finally print and show the wave we wanted =============#\n",
    "print(\"Computing implausibility of NROY...\")\n",
    "wave.find_regions(points_to_emulate)  # enforce the implausibility criterion to detect regions of\n",
    "# non-implausible and of implausible points\n",
    "print(\"Finished\")\n",
    "nimp = len(wave.nimp_idx)\n",
    "imp = len(wave.imp_idx)\n",
    "tests = nimp + imp\n",
    "perc = 100 * nimp / tests\n",
    "\n",
    "if literature_data:\n",
    "    np.savetxt(os.path.join(PROJECT_PATH, input_folder, \"NROY_rel_literature.dat\"), [perc], fmt='%.2f')\n",
    "else:\n",
    "    if sampling_pts_lhd == 0:\n",
    "        np.savetxt(os.path.join(PROJECT_PATH, input_folder, \"NROY_rel_HF_patient\" + str(patient_number) + \"_sd_\" +\n",
    "                                str(sd_magnitude) + \".dat\"), [perc], fmt='%.2f')\n",
    "    else:\n",
    "        np.savetxt(os.path.join(PROJECT_PATH, input_folder, \"NROY_rel_HF_patient\" + str(patient_number) + \"_sd_\" +\n",
    "                                str(sd_magnitude) + \"_lhd_\" + str(int(sampling_pts_lhd)) + \".dat\"), [perc], fmt='%.2f')\n",
    "        \n",
    "\n",
    "####### PLOT_NROY ########\n",
    "\n",
    "# title = \"Initial wave for HF #01 with SD=10%\"\n",
    "\n",
    "# xlabels = np.genfromtxt(os.path.join(PROJECT_PATH, \"param_labels.txt\"), dtype=str, delimiter='\\n')\n",
    "# xlabels = xlabels[:-1]\n",
    "\n",
    "# pathlib.Path(os.path.join(PROJECT_PATH, input_folder, \"figures\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# if literature_data:\n",
    "#     plot_name = \"literature\"\n",
    "# else:\n",
    "#     plot_name = \"patient_HF\" + str(patient_number) + \"_sd_\" + str(sd_magnitude)\n",
    "\n",
    "# print(\"Plotting impl min...\")\n",
    "# postprocessing.plot_wave(W=wave, xlabels=xlabels,\n",
    "#                          filename=os.path.join(PROJECT_PATH, input_folder, \"figures\", \"wave_\" + plot_name + \"_impl_min\"),\n",
    "#                          reduction_function=\"min\",\n",
    "#                          plot_title=title + \": taking the min of each slice\",\n",
    "#                          param_ranges=param_ranges\n",
    "#                          )\n",
    "# print(\"Plotting prob imp...\")\n",
    "# postprocessing.plot_wave(W=wave, xlabels=xlabels,\n",
    "#                          filename=os.path.join(PROJECT_PATH, input_folder, \"figures\", \"wave_\" + plot_name + \"_prob_imp\"),\n",
    "#                          reduction_function=\"prob_IMP\",\n",
    "#                          plot_title=title + \": percentage of implausible points\",\n",
    "#                          param_ranges=param_ranges\n",
    "#                          )\n",
    "\n",
    "####### GENERATE NEW TRAINING POINTS #####\n",
    "\n",
    "num_pts=140\n",
    "output_folder=\"patient_HF\" + str(patient_number) +  \"_sd_\" + str(sd_magnitude)+ \"/wave1\"\n",
    "input_folder=\"initial_sweep\"\n",
    "wave_name=\"wave0_patient_HF\" + str(patient_number) +  \"_sd_\" + str(sd_magnitude)\n",
    "\n",
    "\n",
    "if not os.path.isfile(os.path.join(PROJECT_PATH, output_folder, \"input_space_training.dat\")):\n",
    "\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH, input_folder, \"variance_quotient\" + wave_name + \".dat\")):\n",
    "        np.savetxt(os.path.join(PROJECT_PATH, input_folder, \"variance_quotient_\" + wave_name + \".dat\"), wave.PV,\n",
    "                   fmt=\"%.2f\")\n",
    "    wave.save(os.path.join(PROJECT_PATH, input_folder, wave_name)) # Just in case there are not enough points in the NROY\n",
    "\n",
    "    if num_pts > 0:\n",
    "        training_pts = wave.get_points(num_pts)\n",
    "    else:\n",
    "        training_pts = []\n",
    "\n",
    "    pathlib.Path(os.path.join(PROJECT_PATH, output_folder)).mkdir(parents=True, exist_ok=True)\n",
    "    np.savetxt(os.path.join(PROJECT_PATH, output_folder, \"input_space_training.dat\"), training_pts, fmt=\"%.2f\")\n",
    "\n",
    "    wave.save(os.path.join(PROJECT_PATH, input_folder, wave_name))\n",
    "\n",
    "if not os.path.isfile(os.path.join(PROJECT_PATH, output_folder, \"input_ep_training.dat\")) or not os.path.isfile(os.path.join(PROJECT_PATH, output_folder, \"input_anatomy_training.csv\")):\n",
    "    with open(os.path.join(PROJECT_PATH, output_folder, \"input_space_training.dat\")) as f:\n",
    "        anatomy_and_ep_values = f.read().splitlines()\n",
    "\n",
    "    x_anatomy = []\n",
    "    x_ep = []\n",
    "\n",
    "    for full_line in anatomy_and_ep_values:\n",
    "        if type(anatomy_and_ep_values) is not np.ndarray:\n",
    "            line = full_line.split(' ')\n",
    "        else:\n",
    "            line = full_line\n",
    "        x_anatomy.append(line[0:9])\n",
    "        x_ep.append(line[9:14])\n",
    "\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH, output_folder, \"input_ep_training.dat\")):\n",
    "        f = open(os.path.join(PROJECT_PATH, output_folder, \"input_ep_training.dat\"), \"w\")\n",
    "        if type(anatomy_and_ep_values) is not np.ndarray:\n",
    "            f.writelines(' '.join(row) + '\\n' for row in x_ep)\n",
    "        else:\n",
    "            f.writelines(' '.join(str(elem) for elem in row) + '\\n' for row in x_ep)\n",
    "        f.close()\n",
    "\n",
    "    if not os.path.isfile(os.path.join(PROJECT_PATH, output_folder, \"input_ep_training.csv\")):\n",
    "        with open(os.path.join(PROJECT_PATH, output_folder, \"input_anatomy_training.csv\"), mode='w') as f:\n",
    "            f_writer = csv.writer(f, delimiter=',')\n",
    "\n",
    "            f_writer.writerow([\"Mode\" + str(i) for i in range(1, 19)])\n",
    "\n",
    "            for current_line in range(len(x_anatomy)):\n",
    "                output = np.zeros(18)\n",
    "                output[0:9] = x_anatomy[current_line]\n",
    "                f_writer.writerow([\"{0:.2f}\".format(round(i, 2)) for i in output])\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-hm",
   "language": "python",
   "name": "python3.8-hm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
